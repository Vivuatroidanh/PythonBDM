{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 19:06:35,876 - __main__ - INFO - Models exist in keras format, loading and converting to pickle...\n",
      "2025-04-11 19:06:35,877 - __main__ - INFO - Loading densenet121 model from keras: models\\stroke_densenet121.keras\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 19:06:36,124 - tensorflow - WARNING - From C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 19:06:36,231 - tensorflow - WARNING - From C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "2025-04-11 19:07:56,094 - __main__ - INFO - Successfully loaded densenet121 model from keras\n",
      "2025-04-11 19:07:56,094 - __main__ - INFO - Loading resnet50 model from keras: models\\stroke_resnet50.keras\n",
      "2025-04-11 19:09:52,685 - __main__ - INFO - Successfully loaded resnet50 model from keras\n",
      "2025-04-11 19:09:52,685 - __main__ - INFO - Loading xception model from keras: models\\stroke_xception.keras\n",
      "2025-04-11 19:11:10,257 - __main__ - INFO - Successfully loaded xception model from keras\n",
      "2025-04-11 19:11:10,257 - __main__ - INFO - Saving densenet121 model to pickle: models\\stroke_densenet121.pkl\n",
      "2025-04-11 19:11:13,862 - __main__ - INFO - Successfully saved densenet121 model to pickle\n",
      "2025-04-11 19:11:13,862 - __main__ - INFO - Saving resnet50 model to pickle: models\\stroke_resnet50.pkl\n",
      "2025-04-11 19:11:19,593 - __main__ - INFO - Successfully saved resnet50 model to pickle\n",
      "2025-04-11 19:11:19,593 - __main__ - INFO - Saving xception model to pickle: models\\stroke_xception.pkl\n",
      "2025-04-11 19:11:24,450 - __main__ - INFO - Successfully saved xception model to pickle\n",
      "2025-04-11 19:11:24,450 - __main__ - INFO - Predicting on Dataset\\Normal\\1 (1).jpg...\n",
      "2025-04-11 19:11:30,572 - __main__ - INFO - Predicted class: Normal\n",
      "2025-04-11 19:11:30,572 - __main__ - INFO - Confidence: 0.8866\n",
      "2025-04-11 19:11:30,572 - __main__ - INFO - densenet121: Normal=0.9720, Stroke=0.0280\n",
      "2025-04-11 19:11:30,572 - __main__ - INFO - resnet50: Normal=0.9760, Stroke=0.0240\n",
      "2025-04-11 19:11:30,577 - __main__ - INFO - xception: Normal=0.7117, Stroke=0.2883\n",
      "2025-04-11 19:11:30,579 - __main__ - INFO - Potentially affected brain areas:\n",
      "2025-04-11 19:11:30,579 - __main__ - INFO - - Right hemisphere\n",
      "2025-04-11 19:11:30,580 - __main__ - INFO - - Anterior region\n",
      "2025-04-11 19:11:30,580 - __main__ - INFO - - Possibly parietal lobe\n",
      "2025-04-11 19:11:30,582 - __main__ - INFO - - Note: This is a preliminary estimate and should be confirmed by a medical professional\n",
      "2025-04-11 19:11:30,593 - __main__ - INFO - Heatmap saved to models\\visualizations\\1 (1)_heatmap.png\n",
      "2025-04-11 19:11:30,593 - __main__ - INFO - Predicting on Dataset\\Stroke\\2 (1).jpg...\n",
      "2025-04-11 19:11:32,282 - __main__ - INFO - Predicted class: Stroke\n",
      "2025-04-11 19:11:32,298 - __main__ - INFO - Confidence: 0.7100\n",
      "2025-04-11 19:11:32,298 - __main__ - INFO - densenet121: Normal=0.8700, Stroke=0.1300\n",
      "2025-04-11 19:11:32,298 - __main__ - INFO - resnet50: Normal=0.0001, Stroke=0.9999\n",
      "2025-04-11 19:11:32,303 - __main__ - INFO - xception: Normal=0.0000, Stroke=1.0000\n",
      "2025-04-11 19:11:32,305 - __main__ - INFO - Potentially affected brain areas:\n",
      "2025-04-11 19:11:32,307 - __main__ - INFO - - Left hemisphere\n",
      "2025-04-11 19:11:32,307 - __main__ - INFO - - Anterior region\n",
      "2025-04-11 19:11:32,309 - __main__ - INFO - - Possibly parietal lobe\n",
      "2025-04-11 19:11:32,311 - __main__ - INFO - - Note: This is a preliminary estimate and should be confirmed by a medical professional\n",
      "2025-04-11 19:11:32,317 - __main__ - INFO - Heatmap saved to models\\visualizations\\2 (1)_heatmap.png\n",
      "2025-04-11 19:11:32,318 - __main__ - INFO - Process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import DenseNet121, ResNet50, Xception\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"cnn_models.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_model(base_model_name, input_shape=(256, 256, 3), classes=2):\n",
    "    \"\"\"\n",
    "    Create a CNN model using a pre-trained base model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_model_name : str\n",
    "        Name of the base model to use ('densenet121', 'resnet50', or 'xception')\n",
    "    input_shape : tuple\n",
    "        Input shape for the model\n",
    "    classes : int\n",
    "        Number of output classes\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Model\n",
    "        The compiled Keras model\n",
    "    \"\"\"\n",
    "    if base_model_name.lower() == 'densenet121':\n",
    "        base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    elif base_model_name.lower() == 'resnet50':\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    elif base_model_name.lower() == 'xception':\n",
    "        base_model = Xception(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown base model: {base_model_name}\")\n",
    "    \n",
    "    # Add custom layers on top of the base model\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(classes, activation='softmax')(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, dataset_dir, input_shape=(256, 256), batch_size=32, epochs=20):\n",
    "    \"\"\"\n",
    "    Train a CNN model on the dataset\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : Model\n",
    "        The Keras model to train\n",
    "    dataset_dir : str\n",
    "        Path to the dataset directory\n",
    "    input_shape : tuple\n",
    "        Input shape for images (without channels)\n",
    "    batch_size : int\n",
    "        Batch size for training\n",
    "    epochs : int\n",
    "        Number of epochs to train\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Model\n",
    "        The trained Keras model\n",
    "    History\n",
    "        Training history\n",
    "    \"\"\"\n",
    "    # Create data generators with augmentation\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    \n",
    "    training_generator = train_datagen.flow_from_directory(\n",
    "        dataset_dir,\n",
    "        target_size=input_shape,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "    \n",
    "    validation_generator = train_datagen.flow_from_directory(\n",
    "        dataset_dir,\n",
    "        target_size=input_shape,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        training_generator,\n",
    "        steps_per_epoch=training_generator.samples // batch_size,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_generator.samples // batch_size,\n",
    "        epochs=epochs\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, dataset_dir, input_shape=(256, 256), batch_size=32):\n",
    "    \"\"\"\n",
    "    Evaluate a CNN model on the dataset\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : Model\n",
    "        The Keras model to evaluate\n",
    "    dataset_dir : str\n",
    "        Path to the dataset directory\n",
    "    input_shape : tuple\n",
    "        Input shape for images (without channels)\n",
    "    batch_size : int\n",
    "        Batch size for evaluation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Evaluation metrics\n",
    "    \"\"\"\n",
    "    # Create data generator\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        dataset_dir,\n",
    "        target_size=input_shape,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    metrics = model.evaluate(\n",
    "        test_generator,\n",
    "        steps=test_generator.samples // batch_size + 1\n",
    "    )\n",
    "    \n",
    "    result = dict(zip(model.metrics_names, metrics))\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = model.predict(\n",
    "        test_generator,\n",
    "        steps=test_generator.samples // batch_size + 1\n",
    "    )\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Get true labels\n",
    "    y_true = test_generator.classes\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    cm = confusion_matrix(y_true, y_pred_classes)\n",
    "    report = classification_report(y_true, y_pred_classes, target_names=test_generator.class_indices.keys())\n",
    "    \n",
    "    result['confusion_matrix'] = cm\n",
    "    result['classification_report'] = report\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_ensemble_model(models):\n",
    "    \"\"\"\n",
    "    Create an ensemble model from multiple base models\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : list\n",
    "        List of base models to include in the ensemble\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    function\n",
    "        A function that takes an image and returns ensemble predictions\n",
    "    \"\"\"\n",
    "    def ensemble_predict(img):\n",
    "        \"\"\"\n",
    "        Make an ensemble prediction on a single image\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        img : array\n",
    "            Input image\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        array\n",
    "            Ensemble prediction\n",
    "        \"\"\"\n",
    "        # Preprocess image\n",
    "        img = cv2.resize(img, (256, 256))\n",
    "        img = img / 255.0\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        \n",
    "        # Get predictions from all models\n",
    "        predictions = []\n",
    "        for model in models:\n",
    "            pred = model.predict(img, verbose=0)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        # Average predictions\n",
    "        ensemble_pred = np.mean(predictions, axis=0)\n",
    "        \n",
    "        return ensemble_pred\n",
    "    \n",
    "    return ensemble_predict\n",
    "\n",
    "def generate_gradcam(img, model, layer_name=None):\n",
    "    \"\"\"\n",
    "    Generate a GradCAM heatmap for the image\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    img : array\n",
    "        Input image (normalized)\n",
    "    model : Model\n",
    "        The Keras model to use\n",
    "    layer_name : str or None\n",
    "        Name of the layer to use for GradCAM\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    array\n",
    "        GradCAM heatmap\n",
    "    \"\"\"\n",
    "    # Expand dimensions to match model input shape\n",
    "    img_tensor = np.expand_dims(img, axis=0)\n",
    "    \n",
    "    # If layer_name is not provided, try to find the last convolutional layer\n",
    "    if layer_name is None:\n",
    "        for layer in reversed(model.layers):\n",
    "            if 'conv' in layer.name:\n",
    "                layer_name = layer.name\n",
    "                break\n",
    "    \n",
    "    # If we still don't have a layer_name, raise an error\n",
    "    if layer_name is None:\n",
    "        raise ValueError(\"Could not find a convolutional layer in the model\")\n",
    "    \n",
    "    # Get the gradient of the top predicted class with respect to the output of the layer\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        inputs=[model.inputs],\n",
    "        outputs=[model.get_layer(layer_name).output, model.output]\n",
    "    )\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # Cast the image tensor to a float-32 tensor\n",
    "        img_tensor = tf.cast(img_tensor, tf.float32)\n",
    "        conv_outputs, predictions = grad_model(img_tensor)\n",
    "        class_idx = tf.argmax(predictions[0])\n",
    "        top_class = predictions[:, class_idx]\n",
    "    \n",
    "    # Compute the gradient of the top predicted class with respect to the activations of the last conv layer\n",
    "    grads = tape.gradient(top_class, conv_outputs)\n",
    "    \n",
    "    # This is a vector where each entry is the mean intensity of the gradient over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    \n",
    "    # Multiply each channel in the feature map array by the \"importance\" of this channel\n",
    "    # FIX: Use TensorFlow operations instead of direct assignment\n",
    "    conv_outputs_tensor = conv_outputs[0].numpy()  # Convert to numpy for manipulation\n",
    "    \n",
    "    # Weight feature maps by importance using NumPy\n",
    "    for i in range(pooled_grads.shape[0]):\n",
    "        conv_outputs_tensor[:, :, i] *= pooled_grads[i].numpy()\n",
    "    \n",
    "    # The channel-wise mean of the weighted feature map\n",
    "    heatmap = np.mean(conv_outputs_tensor, axis=-1)\n",
    "    \n",
    "    # For visualization purpose, normalize the heatmap between 0 & 1\n",
    "    heatmap = np.maximum(heatmap, 0) / (np.max(heatmap) or 1e-10)\n",
    "    \n",
    "    # Resize to original image size\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    \n",
    "    return heatmap\n",
    "\n",
    "def superimpose_heatmap(img, heatmap, alpha=0.6):\n",
    "    \"\"\"\n",
    "    Superimpose a heatmap on the original image\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    img : array\n",
    "        Original image\n",
    "    heatmap : array\n",
    "        GradCAM heatmap\n",
    "    alpha : float\n",
    "        Transparency factor\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    array\n",
    "        Superimposed image\n",
    "    \"\"\"\n",
    "    # Convert heatmap to RGB\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    \n",
    "    # Convert original image to uint8 if it's not already\n",
    "    if img.dtype != np.uint8:\n",
    "        img = np.uint8(255 * img)\n",
    "    \n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = cv2.addWeighted(img, 1.0 - alpha, heatmap, alpha, 0)\n",
    "    \n",
    "    return superimposed_img\n",
    "\n",
    "def identify_affected_areas(heatmap, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Identify affected brain areas based on the heatmap\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    heatmap : array\n",
    "        GradCAM heatmap\n",
    "    threshold : float\n",
    "        Threshold for considering a region as affected\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        Names of potentially affected brain areas\n",
    "    \"\"\"\n",
    "    # Binarize the heatmap using the threshold\n",
    "    binary_heatmap = heatmap > threshold\n",
    "    \n",
    "    # Calculate the centroid of the affected area\n",
    "    y_indices, x_indices = np.where(binary_heatmap)\n",
    "    if len(y_indices) == 0 or len(x_indices) == 0:\n",
    "        return [\"No specific affected areas detected\"]\n",
    "    \n",
    "    centroid_y = np.mean(y_indices)\n",
    "    centroid_x = np.mean(x_indices)\n",
    "    \n",
    "    # Brain region mapping based on image quadrants\n",
    "    h, w = heatmap.shape\n",
    "    regions = []\n",
    "    \n",
    "    # Left vs Right\n",
    "    if centroid_x < w/2:\n",
    "        regions.append(\"Left hemisphere\")\n",
    "    else:\n",
    "        regions.append(\"Right hemisphere\")\n",
    "    \n",
    "    # Anterior vs Posterior\n",
    "    if centroid_y < h/2:\n",
    "        regions.append(\"Anterior region\")\n",
    "    else:\n",
    "        regions.append(\"Posterior region\")\n",
    "    \n",
    "    # Check specific quadrants for more detailed localization\n",
    "    if centroid_x < w/3:\n",
    "        if centroid_y < h/3:\n",
    "            regions.append(\"Possibly frontal lobe\")\n",
    "        elif centroid_y > 2*h/3:\n",
    "            regions.append(\"Possibly occipital lobe\")\n",
    "        else:\n",
    "            regions.append(\"Possibly temporal lobe\")\n",
    "    elif centroid_x > 2*w/3:\n",
    "        if centroid_y < h/3:\n",
    "            regions.append(\"Possibly frontal lobe\")\n",
    "        elif centroid_y > 2*h/3:\n",
    "            regions.append(\"Possibly occipital lobe\")\n",
    "        else:\n",
    "            regions.append(\"Possibly temporal lobe\")\n",
    "    else:\n",
    "        if centroid_y < h/2:\n",
    "            regions.append(\"Possibly parietal lobe\")\n",
    "        else:\n",
    "            regions.append(\"Possibly cerebellum or brain stem\")\n",
    "    \n",
    "    # Add a disclaimer\n",
    "    regions.append(\"Note: This is a preliminary estimate and should be confirmed by a medical professional\")\n",
    "    \n",
    "    return regions\n",
    "\n",
    "def predict_stroke(image_path, models=None, models_dir='models'):\n",
    "    \"\"\"\n",
    "    Predict stroke from a brain CT scan image\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        Path to the input image\n",
    "    models : dict or None\n",
    "        Dictionary of pre-loaded models\n",
    "    models_dir : str\n",
    "        Directory containing saved models\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Prediction results including class, probability, and heatmap\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Failed to load image: {image_path}\")\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize image\n",
    "        img_resized = cv2.resize(img, (256, 256))\n",
    "        \n",
    "        # Normalize image\n",
    "        img_normalized = img_resized / 255.0\n",
    "        \n",
    "        # Expand dimensions to match model input shape\n",
    "        img_batch = np.expand_dims(img_normalized, axis=0)\n",
    "        \n",
    "        # Load models if not provided\n",
    "        if models is None:\n",
    "            models = {}\n",
    "            model_names = ['densenet121', 'resnet50', 'xception']\n",
    "            \n",
    "            for model_name in model_names:\n",
    "                # Try loading from pickle first\n",
    "                pickle_path = os.path.join(models_dir, f\"stroke_{model_name}.pkl\")\n",
    "                keras_path = os.path.join(models_dir, f\"stroke_{model_name}.keras\")\n",
    "                \n",
    "                if os.path.exists(pickle_path):\n",
    "                    with open(pickle_path, 'rb') as f:\n",
    "                        models[model_name] = pickle.load(f)\n",
    "                elif os.path.exists(keras_path):\n",
    "                    models[model_name] = tf.keras.models.load_model(keras_path)\n",
    "        \n",
    "        if not models:\n",
    "            raise ValueError(\"No models available for prediction\")\n",
    "        \n",
    "        # Make predictions with each model\n",
    "        predictions = {}\n",
    "        for model_name, model in models.items():\n",
    "            pred = model.predict(img_batch, verbose=0)[0]\n",
    "            predictions[model_name] = {\n",
    "                'normal_prob': float(pred[0]),\n",
    "                'stroke_prob': float(pred[1])\n",
    "            }\n",
    "        \n",
    "        # Calculate ensemble prediction (average of all models)\n",
    "        ensemble_normal_prob = np.mean([pred['normal_prob'] for pred in predictions.values()])\n",
    "        ensemble_stroke_prob = np.mean([pred['stroke_prob'] for pred in predictions.values()])\n",
    "        \n",
    "        # Determine the predicted class\n",
    "        predicted_class = 'Stroke' if ensemble_stroke_prob > 0.5 else 'Normal'\n",
    "        \n",
    "        # Generate heatmap for the most confident model\n",
    "        most_confident_model_name = max(\n",
    "            predictions.keys(), \n",
    "            key=lambda k: predictions[k]['stroke_prob'] if predicted_class == 'Stroke' else predictions[k]['normal_prob']\n",
    "        )\n",
    "        most_confident_model = models[most_confident_model_name]\n",
    "        \n",
    "        # Create GradCAM for visualization\n",
    "        heatmap = generate_gradcam(img_normalized, most_confident_model)\n",
    "        \n",
    "        # Create superimposed image\n",
    "        superimposed_img = superimpose_heatmap(img_resized, heatmap)\n",
    "        \n",
    "        # Identify affected areas\n",
    "        affected_areas = identify_affected_areas(heatmap)\n",
    "        \n",
    "        result = {\n",
    "            'predicted_class': predicted_class,\n",
    "            'confidence': float(ensemble_stroke_prob if predicted_class == 'Stroke' else ensemble_normal_prob),\n",
    "            'model_predictions': predictions,\n",
    "            'heatmap_image': superimposed_img,\n",
    "            'heatmap': heatmap,\n",
    "            'affected_areas': affected_areas\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in predict_stroke: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def load_models(models_dir='models'):\n",
    "    \"\"\"\n",
    "    Load trained CNN models\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models_dir : str\n",
    "        Directory containing the saved models\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary of loaded models\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    model_names = ['densenet121', 'resnet50', 'xception']\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        # Try loading from pickle first\n",
    "        pickle_path = os.path.join(models_dir, f\"stroke_{model_name}.pkl\")\n",
    "        keras_path = os.path.join(models_dir, f\"stroke_{model_name}.keras\")\n",
    "        \n",
    "        if os.path.exists(pickle_path):\n",
    "            logger.info(f\"Loading {model_name} model from pickle: {pickle_path}\")\n",
    "            try:\n",
    "                with open(pickle_path, 'rb') as f:\n",
    "                    models[model_name] = pickle.load(f)\n",
    "                logger.info(f\"Successfully loaded {model_name} model from pickle\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading {model_name} model from pickle: {str(e)}\")\n",
    "        \n",
    "        elif os.path.exists(keras_path):\n",
    "            logger.info(f\"Loading {model_name} model from keras: {keras_path}\")\n",
    "            try:\n",
    "                models[model_name] = tf.keras.models.load_model(keras_path)\n",
    "                logger.info(f\"Successfully loaded {model_name} model from keras\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading {model_name} model: {str(e)}\")\n",
    "    \n",
    "    if not models:\n",
    "        logger.warning(\"No models were loaded. Make sure the model files exist.\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "def save_models_as_pickle(models, models_dir='models'):\n",
    "    \"\"\"\n",
    "    Save models in pickle format for use in the web app\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : dict\n",
    "        Dictionary of models to save\n",
    "    models_dir : str\n",
    "        Directory to save the models\n",
    "    \"\"\"\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        pickle_path = os.path.join(models_dir, f\"stroke_{model_name}.pkl\")\n",
    "        logger.info(f\"Saving {model_name} model to pickle: {pickle_path}\")\n",
    "        \n",
    "        try:\n",
    "            with open(pickle_path, 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "            logger.info(f\"Successfully saved {model_name} model to pickle\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving {model_name} model to pickle: {str(e)}\")\n",
    "\n",
    "def train_and_save_all_models(dataset_dir, models_dir='models'):\n",
    "    \"\"\"\n",
    "    Train and save all three models (DenseNet121, ResNet50, Xception)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_dir : str\n",
    "        Path to the dataset directory\n",
    "    models_dir : str\n",
    "        Directory to save the models\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing trained models\n",
    "    \"\"\"\n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    \n",
    "    # Models to train\n",
    "    model_names = ['densenet121', 'resnet50', 'xception']\n",
    "    trained_models = {}\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        logger.info(f\"Training {model_name.upper()} model...\")\n",
    "        \n",
    "        # Create and train the model\n",
    "        model = create_model(model_name)\n",
    "        model, history = train_model(model, dataset_dir)\n",
    "        \n",
    "        # Save the model in keras format\n",
    "        keras_path = os.path.join(models_dir, f\"stroke_{model_name}.keras\")\n",
    "        model.save(keras_path)\n",
    "        logger.info(f\"Model saved to {keras_path}\")\n",
    "        \n",
    "        # Also save as pickle for web app\n",
    "        pickle_path = os.path.join(models_dir, f\"stroke_{model_name}.pkl\")\n",
    "        with open(pickle_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        logger.info(f\"Model saved as pickle to {pickle_path}\")\n",
    "        \n",
    "        # Plot and save training history\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'], label='Train')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "        plt.title(f'{model_name.upper()} - Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'], label='Train')\n",
    "        plt.plot(history.history['val_loss'], label='Validation')\n",
    "        plt.title(f'{model_name.upper()} - Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(models_dir, f\"{model_name}_history.png\"))\n",
    "        plt.close()\n",
    "        \n",
    "        # Store the trained model\n",
    "        trained_models[model_name] = model\n",
    "        \n",
    "        # Evaluate the model\n",
    "        logger.info(f\"Evaluating {model_name.upper()} model...\")\n",
    "        result = evaluate_model(model, dataset_dir)\n",
    "        \n",
    "        logger.info(f\"Accuracy: {result['accuracy']:.4f}\")\n",
    "        logger.info(f\"Loss: {result['loss']:.4f}\")\n",
    "        logger.info(f\"Confusion Matrix:\\n{result['confusion_matrix']}\")\n",
    "        logger.info(f\"Classification Report:\\n{result['classification_report']}\")\n",
    "    \n",
    "    return trained_models\n",
    "\n",
    "def test_prediction_on_samples(trained_models, dataset_dir, models_dir='models'):\n",
    "    \"\"\"\n",
    "    Test the prediction on sample images\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    trained_models : dict\n",
    "        Dictionary of trained models\n",
    "    dataset_dir : str\n",
    "        Path to the dataset directory\n",
    "    models_dir : str\n",
    "        Directory containing saved models\n",
    "    \"\"\"\n",
    "    # Test prediction on sample images\n",
    "    sample_paths = [\n",
    "        os.path.join(dataset_dir, 'Normal', '1 (1).jpg'),\n",
    "        os.path.join(dataset_dir, 'Stroke', '2 (1).jpg')\n",
    "    ]\n",
    "    \n",
    "    for sample_path in sample_paths:\n",
    "        if os.path.exists(sample_path):\n",
    "            logger.info(f\"Predicting on {sample_path}...\")\n",
    "            try:\n",
    "                result = predict_stroke(sample_path, trained_models)\n",
    "                \n",
    "                logger.info(f\"Predicted class: {result['predicted_class']}\")\n",
    "                logger.info(f\"Confidence: {result['confidence']:.4f}\")\n",
    "                \n",
    "                for model_name, pred in result['model_predictions'].items():\n",
    "                    logger.info(f\"{model_name}: Normal={pred['normal_prob']:.4f}, Stroke={pred['stroke_prob']:.4f}\")\n",
    "                \n",
    "                # Log affected areas\n",
    "                logger.info(\"Potentially affected brain areas:\")\n",
    "                for area in result['affected_areas']:\n",
    "                    logger.info(f\"- {area}\")\n",
    "                \n",
    "                # Save visualization\n",
    "                output_dir = os.path.join(models_dir, 'visualizations')\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                \n",
    "                # Get filename without path and extension\n",
    "                filename = os.path.splitext(os.path.basename(sample_path))[0]\n",
    "                \n",
    "                # Save heatmap image\n",
    "                heatmap_path = os.path.join(output_dir, f\"{filename}_heatmap.png\")\n",
    "                cv2.imwrite(heatmap_path, cv2.cvtColor(result['heatmap_image'], cv2.COLOR_RGB2BGR))\n",
    "                logger.info(f\"Heatmap saved to {heatmap_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error predicting on {sample_path}: {str(e)}\")\n",
    "        else:\n",
    "            logger.warning(f\"Sample image not found: {sample_path}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to train and evaluate models\n",
    "    \"\"\"\n",
    "    dataset_dir = 'Dataset'\n",
    "    models_dir = 'models'\n",
    "    \n",
    "    # Check if models directory exists\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    \n",
    "    # Check if models already exist in pickle format\n",
    "    pickle_files = [f\"stroke_{name}.pkl\" for name in ['densenet121', 'resnet50', 'xception']]\n",
    "    pickles_exist = all(os.path.exists(os.path.join(models_dir, f)) for f in pickle_files)\n",
    "    \n",
    "    # Check if models exist in keras format\n",
    "    keras_files = [f\"stroke_{name}.keras\" for name in ['densenet121', 'resnet50', 'xception']]\n",
    "    keras_exist = all(os.path.exists(os.path.join(models_dir, f)) for f in keras_files)\n",
    "    \n",
    "    trained_models = {}\n",
    "    \n",
    "    if pickles_exist:\n",
    "        logger.info(\"Models already exist in pickle format, loading them...\")\n",
    "        trained_models = load_models(models_dir)\n",
    "    elif keras_exist:\n",
    "        logger.info(\"Models exist in keras format, loading and converting to pickle...\")\n",
    "        trained_models = load_models(models_dir)\n",
    "        save_models_as_pickle(trained_models, models_dir)\n",
    "    else:\n",
    "        logger.info(\"Training new models...\")\n",
    "        trained_models = train_and_save_all_models(dataset_dir, models_dir)\n",
    "    \n",
    "    # Test prediction on sample images\n",
    "    test_prediction_on_samples(trained_models, dataset_dir, models_dir)\n",
    "    \n",
    "    logger.info(\"Process completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
