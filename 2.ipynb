{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Comprehensive Stroke Prediction with Multiple Models and SHAP Explainability\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# For data preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n",
    "\n",
    "# For handling imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# For metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, confusion_matrix, classification_report, \n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "# Models to evaluate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# For SHAP explanations\n",
    "import shap\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create directories for outputs\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "os.makedirs('shap_plots', exist_ok=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def load_and_explore_data(file_path=\"healthcare-dataset-stroke-data.csv\"):\n",
    "    \"\"\"\n",
    "    Load and perform exploratory data analysis on the stroke dataset\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the dataset\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Processed dataframe\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 1: LOADING AND EXPLORING DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load the dataset\n",
    "    print(f\"\\nLoading data from {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\nDataset shape: {df.shape}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(\"\\nMissing values by column:\")\n",
    "    print(missing_values[missing_values > 0])\n",
    "    \n",
    "    # Target distribution\n",
    "    print(\"\\nTarget (stroke) distribution:\")\n",
    "    stroke_counts = df['stroke'].value_counts()\n",
    "    print(stroke_counts)\n",
    "    print(f\"Percentage of stroke cases: {100 * stroke_counts[1] / len(df):.2f}%\")\n",
    "    \n",
    "    # Feature distributions\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Plot target distribution\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.countplot(x='stroke', data=df, palette='Set2')\n",
    "    plt.title('Stroke Distribution')\n",
    "    \n",
    "    # Plot age distribution by stroke\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.boxplot(x='stroke', y='age', data=df)\n",
    "    plt.title('Age Distribution by Stroke Status')\n",
    "    \n",
    "    # Plot glucose level distribution by stroke\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.boxplot(x='stroke', y='avg_glucose_level', data=df)\n",
    "    plt.title('Glucose Level by Stroke Status')\n",
    "    \n",
    "    # Plot BMI distribution by stroke\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.boxplot(x='stroke', y='bmi', data=df)\n",
    "    plt.title('BMI by Stroke Status')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/exploratory_analysis.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    print(f\"\\nNumber of duplicated rows: {duplicate_count}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Perform data preprocessing including handling missing values,\n",
    "    encoding categorical features, and preparing features and target\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Raw dataframe\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        X, y, encoder, and scaler\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 2: DATA PREPROCESSING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Drop ID column if it exists\n",
    "    if 'id' in data.columns:\n",
    "        print(\"\\nDropping ID column...\")\n",
    "        data.drop('id', axis=1, inplace=True)\n",
    "    \n",
    "    # Handle the 'Other' gender category (very small number of samples)\n",
    "    if 'gender' in data.columns:\n",
    "        other_count = (data['gender'] == 'Other').sum()\n",
    "        if other_count > 0:\n",
    "            print(f\"\\nReplacing 'Other' gender category ({other_count} samples) with 'Female'...\")\n",
    "            data['gender'] = data['gender'].replace('Other', 'Female')\n",
    "    \n",
    "    # Handle missing values in bmi using KNN imputation\n",
    "    if 'bmi' in data.columns and data['bmi'].isnull().sum() > 0:\n",
    "        missing_bmi_count = data['bmi'].isnull().sum()\n",
    "        print(f\"\\nImputing {missing_bmi_count} missing BMI values using KNN...\")\n",
    "        \n",
    "        # Prepare data for KNN imputation\n",
    "        impute_df = data.copy()\n",
    "        \n",
    "        # Encode categorical features for KNN imputation\n",
    "        categorical_cols = impute_df.select_dtypes(include=['object']).columns\n",
    "        for col in categorical_cols:\n",
    "            impute_df[col] = pd.factorize(impute_df[col])[0]\n",
    "        \n",
    "        # Apply KNN imputation\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        imputed_data = imputer.fit_transform(impute_df)\n",
    "        imputed_df = pd.DataFrame(imputed_data, columns=impute_df.columns)\n",
    "        \n",
    "        # Update BMI values in original dataframe\n",
    "        data['bmi'] = imputed_df['bmi']\n",
    "    \n",
    "    # Handle categorical features\n",
    "    print(\"\\nEncoding categorical features...\")\n",
    "    \n",
    "    # Create dummies for categorical features\n",
    "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Apply one-hot encoding\n",
    "    data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    # Check for outliers in numerical features\n",
    "    print(\"\\nChecking for outliers in numerical features...\")\n",
    "    numerical_cols = ['age', 'avg_glucose_level', 'bmi']\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        # Calculate IQR\n",
    "        Q1 = data[col].quantile(0.25)\n",
    "        Q3 = data[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Define bounds\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Count outliers\n",
    "        outliers = ((data[col] < lower_bound) | (data[col] > upper_bound)).sum()\n",
    "        print(f\"  {col}: {outliers} outliers detected\")\n",
    "    \n",
    "    # Split into features and target\n",
    "    print(\"\\nSplitting into features and target...\")\n",
    "    X = data.drop('stroke', axis=1)\n",
    "    y = data['stroke']\n",
    "    \n",
    "    # Scale numerical features\n",
    "    print(\"\\nScaling numerical features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "    \n",
    "    print(f\"\\nFinal feature set: {X.shape[1]} features\")\n",
    "    print(f\"Feature names: {X.columns.tolist()}\")\n",
    "    \n",
    "    return X, y, scaler\n",
    "\n",
    "def handle_imbalance(X, y, method='smote'):\n",
    "    \"\"\"\n",
    "    Handle class imbalance using SMOTE\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pd.DataFrame\n",
    "        Feature dataframe\n",
    "    y : pd.Series\n",
    "        Target series\n",
    "    method : str\n",
    "        Method to use for handling imbalance\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Balanced X and y\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 3: HANDLING CLASS IMBALANCE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Display class distribution before balancing\n",
    "    print(\"\\nClass distribution before balancing:\")\n",
    "    class_counts = y.value_counts()\n",
    "    print(class_counts)\n",
    "    print(f\"Class imbalance ratio: 1:{class_counts[0]/class_counts[1]:.1f}\")\n",
    "    \n",
    "    # Apply SMOTE for oversampling\n",
    "    print(f\"\\nApplying {method.upper()} to balance classes...\")\n",
    "    if method.lower() == 'smote':\n",
    "        smote = SMOTE(random_state=RANDOM_SEED)\n",
    "        X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    # Display new class distribution\n",
    "    print(\"\\nClass distribution after balancing:\")\n",
    "    balanced_counts = pd.Series(y_balanced).value_counts()\n",
    "    print(balanced_counts)\n",
    "    \n",
    "    # Plot class distribution before and after balancing\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(['No Stroke', 'Stroke'], class_counts.values, color=['skyblue', 'salmon'])\n",
    "    plt.title('Class Distribution Before Balancing')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(['No Stroke', 'Stroke'], balanced_counts.values, color=['skyblue', 'salmon'])\n",
    "    plt.title('Class Distribution After Balancing')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/class_balance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "def train_and_evaluate_models(X, y, cv=5):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple classification models\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pd.DataFrame\n",
    "        Feature dataframe\n",
    "    y : pd.Series\n",
    "        Target series\n",
    "    cv : int\n",
    "        Number of cross-validation folds\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Results for each model\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 4: MODEL TRAINING AND EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Define models with optimized hyperparameters\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(\n",
    "            C=1.0, \n",
    "            class_weight='balanced',\n",
    "            max_iter=1000,\n",
    "            solver='liblinear',\n",
    "            random_state=RANDOM_SEED\n",
    "        ),\n",
    "        'Decision Tree': DecisionTreeClassifier(\n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            class_weight='balanced',\n",
    "            random_state=RANDOM_SEED\n",
    "        ),\n",
    "        'Random Forest': RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            class_weight='balanced',\n",
    "            random_state=RANDOM_SEED\n",
    "        ),\n",
    "        'KNN': KNeighborsClassifier(\n",
    "            n_neighbors=7,\n",
    "            weights='distance'\n",
    "        ),\n",
    "        'XGBoost': xgb.XGBClassifier(\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=100,\n",
    "            max_depth=4,\n",
    "            min_child_weight=2,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            scale_pos_weight=10,  # Helps with class imbalance\n",
    "            random_state=RANDOM_SEED\n",
    "        ),\n",
    "        'LightGBM': lgb.LGBMClassifier(\n",
    "            learning_rate=0.05,\n",
    "            n_estimators=100,\n",
    "            num_leaves=31,\n",
    "            max_depth=5,\n",
    "            min_child_samples=20,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            is_unbalance=True,  # Handle class imbalance\n",
    "            random_state=RANDOM_SEED\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "    model_performances = {}\n",
    "    \n",
    "    # Split data for validation set (to be used for SHAP explanations)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Train and evaluate each model with cross-validation\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n{'-'*50}\")\n",
    "        print(f\"Training and evaluating {name}...\")\n",
    "        \n",
    "        # Set up cross-validation\n",
    "        skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_SEED)\n",
    "        \n",
    "        # Lists to store metrics from each fold\n",
    "        accuracies = []\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1_scores = []\n",
    "        roc_aucs = []\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        for train_idx, test_idx in skf.split(X_train, y_train):\n",
    "            # Split data for this fold\n",
    "            X_train_fold, X_test_fold = X_train.iloc[train_idx], X_train.iloc[test_idx]\n",
    "            y_train_fold, y_test_fold = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "            \n",
    "            # Train model\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_test_fold)\n",
    "            \n",
    "            try:\n",
    "                # Get probabilities for AUC calculation\n",
    "                y_pred_proba = model.predict_proba(X_test_fold)[:, 1]\n",
    "                has_proba = True\n",
    "            except:\n",
    "                has_proba = False\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracies.append(accuracy_score(y_test_fold, y_pred))\n",
    "            precisions.append(precision_score(y_test_fold, y_pred))\n",
    "            recalls.append(recall_score(y_test_fold, y_pred))\n",
    "            f1_scores.append(f1_score(y_test_fold, y_pred))\n",
    "            \n",
    "            if has_proba:\n",
    "                roc_aucs.append(roc_auc_score(y_test_fold, y_pred_proba))\n",
    "        \n",
    "        # Calculate mean metrics\n",
    "        mean_accuracy = np.mean(accuracies)\n",
    "        mean_precision = np.mean(precisions)\n",
    "        mean_recall = np.mean(recalls)\n",
    "        mean_f1 = np.mean(f1_scores)\n",
    "        mean_roc_auc = np.mean(roc_aucs) if roc_aucs else None\n",
    "        \n",
    "        # Display mean metrics\n",
    "        print(f\"\\nCross-validation results ({cv} folds):\")\n",
    "        print(f\"  Accuracy:  {mean_accuracy:.4f}\")\n",
    "        print(f\"  Precision: {mean_precision:.4f}\")\n",
    "        print(f\"  Recall:    {mean_recall:.4f}\")\n",
    "        print(f\"  F1 Score:  {mean_f1:.4f}\")\n",
    "        if mean_roc_auc:\n",
    "            print(f\"  ROC AUC:   {mean_roc_auc:.4f}\")\n",
    "        \n",
    "        # Train final model on full training set\n",
    "        print(\"\\nTraining final model on full training set...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        \n",
    "        try:\n",
    "            y_val_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "            val_roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "            has_val_proba = True\n",
    "        except:\n",
    "            val_roc_auc = None\n",
    "            has_val_proba = False\n",
    "        \n",
    "        val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        val_precision = precision_score(y_val, y_val_pred)\n",
    "        val_recall = recall_score(y_val, y_val_pred)\n",
    "        val_f1 = f1_score(y_val, y_val_pred)\n",
    "        \n",
    "        print(\"\\nValidation set results:\")\n",
    "        print(f\"  Accuracy:  {val_accuracy:.4f}\")\n",
    "        print(f\"  Precision: {val_precision:.4f}\")\n",
    "        print(f\"  Recall:    {val_recall:.4f}\")\n",
    "        print(f\"  F1 Score:  {val_f1:.4f}\")\n",
    "        if val_roc_auc:\n",
    "            print(f\"  ROC AUC:   {val_roc_auc:.4f}\")\n",
    "        \n",
    "        # Print classification report\n",
    "        print(\"\\nClassification Report (Validation Set):\")\n",
    "        print(classification_report(y_val, y_val_pred))\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm = confusion_matrix(y_val, y_val_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=['No Stroke', 'Stroke'],\n",
    "                   yticklabels=['No Stroke', 'Stroke'])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'Confusion Matrix - {name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/{name.lower().replace(\" \", \"_\")}_confusion_matrix.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot ROC curve if probabilities are available\n",
    "        if has_val_proba:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            fpr, tpr, _ = roc_curve(y_val, y_val_pred_proba)\n",
    "            plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {val_roc_auc:.3f})')\n",
    "            plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title(f'ROC Curve - {name}')\n",
    "            plt.legend(loc='lower right')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.savefig(f'plots/{name.lower().replace(\" \", \"_\")}_roc_curve.png')\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot precision-recall curve\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            precision_vals, recall_vals, _ = precision_recall_curve(y_val, y_val_pred_proba)\n",
    "            avg_precision = average_precision_score(y_val, y_val_pred_proba)\n",
    "            plt.plot(recall_vals, precision_vals, label=f'AP = {avg_precision:.3f}')\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.title(f'Precision-Recall Curve - {name}')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.savefig(f'plots/{name.lower().replace(\" \", \"_\")}_pr_curve.png')\n",
    "            plt.close()\n",
    "        \n",
    "        # Feature importance if available\n",
    "        if hasattr(model, 'feature_importances_') or name == 'Logistic Regression':\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            if name == 'Logistic Regression':\n",
    "                # For logistic regression, use coefficients\n",
    "                coefs = model.coef_[0]\n",
    "                feature_importance = pd.DataFrame({\n",
    "                    'Feature': X.columns,\n",
    "                    'Importance': np.abs(coefs)  # Use absolute values\n",
    "                }).sort_values('Importance', ascending=False)\n",
    "                \n",
    "                # Plot feature importance\n",
    "                sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "                plt.title(f'Feature Importance (Coefficient Magnitude) - {name}')\n",
    "                \n",
    "            else:\n",
    "                # For tree-based models, use built-in feature importance\n",
    "                feature_importance = pd.DataFrame({\n",
    "                    'Feature': X.columns,\n",
    "                    'Importance': model.feature_importances_\n",
    "                }).sort_values('Importance', ascending=False)\n",
    "                \n",
    "                # Plot feature importance\n",
    "                sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "                plt.title(f'Feature Importance - {name}')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'plots/{name.lower().replace(\" \", \"_\")}_feature_importance.png')\n",
    "            plt.close()\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'cv_metrics': {\n",
    "                'accuracy': mean_accuracy,\n",
    "                'precision': mean_precision,\n",
    "                'recall': mean_recall,\n",
    "                'f1': mean_f1,\n",
    "                'roc_auc': mean_roc_auc\n",
    "            },\n",
    "            'val_metrics': {\n",
    "                'accuracy': val_accuracy,\n",
    "                'precision': val_precision,\n",
    "                'recall': val_recall,\n",
    "                'f1': val_f1,\n",
    "                'roc_auc': val_roc_auc\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Store performance metrics for comparison\n",
    "        model_performances[name] = {\n",
    "            'CV Accuracy': mean_accuracy,\n",
    "            'CV Precision': mean_precision,\n",
    "            'CV Recall': mean_recall,\n",
    "            'CV F1': mean_f1,\n",
    "            'CV ROC AUC': mean_roc_auc if mean_roc_auc else 0,\n",
    "            'Val Accuracy': val_accuracy,\n",
    "            'Val Precision': val_precision,\n",
    "            'Val Recall': val_recall,\n",
    "            'Val F1': val_f1,\n",
    "            'Val ROC AUC': val_roc_auc if val_roc_auc else 0\n",
    "        }\n",
    "    \n",
    "    # Create a dataframe for model comparison\n",
    "    performance_df = pd.DataFrame(model_performances).T\n",
    "    \n",
    "    # Sort by F1 score\n",
    "    performance_df = performance_df.sort_values('Val F1', ascending=False)\n",
    "    \n",
    "    # Display model comparison\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL COMPARISON (SORTED BY VALIDATION F1 SCORE)\")\n",
    "    print(\"=\"*80)\n",
    "    print(performance_df.round(4))\n",
    "    \n",
    "    # Plot model comparison\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    performance_df[['Val Accuracy', 'Val Precision', 'Val Recall', 'Val F1', 'Val ROC AUC']].plot(kind='bar')\n",
    "    plt.title('Model Performance Comparison (Validation Set)')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/model_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Return results and the validation data for SHAP analysis\n",
    "    return results, performance_df, (X_val, y_val)\n",
    "\n",
    "def create_shap_explanations(results, X_val, feature_names):\n",
    "    \"\"\"\n",
    "    Create SHAP explanations for the best models\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Results for each model\n",
    "    X_val : pd.DataFrame\n",
    "        Validation feature dataframe\n",
    "    feature_names : list\n",
    "        List of feature names\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 5: MODEL EXPLAINABILITY WITH SHAP\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Models to explain (top 3 based on F1 score)\n",
    "    models_to_explain = list(results.keys())[:3]\n",
    "    \n",
    "    for model_name in models_to_explain:\n",
    "        print(f\"\\nGenerating SHAP explanations for {model_name}...\")\n",
    "        \n",
    "        model = results[model_name]['model']\n",
    "        \n",
    "        try:\n",
    "            # Create explainer based on model type\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                # For tree-based models (Random Forest, XGBoost, etc.)\n",
    "                explainer = shap.TreeExplainer(model)\n",
    "                shap_values = explainer.shap_values(X_val)\n",
    "                \n",
    "                # For classification with 2 classes, shap_values is a list with values for each class\n",
    "                if isinstance(shap_values, list):\n",
    "                    # Use values for positive class (stroke = 1)\n",
    "                    shap_values_to_plot = shap_values[1]\n",
    "                else:\n",
    "                    shap_values_to_plot = shap_values\n",
    "                \n",
    "            else:\n",
    "                # For other models (Logistic Regression, SVM, etc.)\n",
    "                # Create a background dataset (sample from validation set)\n",
    "                background = shap.sample(X_val, 100)\n",
    "                explainer = shap.KernelExplainer(model.predict_proba, background)\n",
    "                shap_values = explainer.shap_values(X_val.iloc[:100])  # Use subset for performance\n",
    "                \n",
    "                # Use values for positive class (stroke = 1)\n",
    "                shap_values_to_plot = shap_values[1]\n",
    "            \n",
    "            # Summary plot (beeswarm plot)\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            shap.summary_plot(shap_values_to_plot, X_val, feature_names=feature_names, show=False)\n",
    "            plt.title(f'SHAP Summary Plot - {model_name}')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'shap_plots/{model_name.lower().replace(\" \", \"_\")}_summary.png')\n",
    "            plt.close()\n",
    "            print(f\"  Summary plot saved to shap_plots/{model_name.lower().replace(' ', '_')}_summary.png\")\n",
    "            \n",
    "            # Bar plot (feature importance)\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            shap.summary_plot(shap_values_to_plot, X_val, feature_names=feature_names, plot_type='bar', show=False)\n",
    "            plt.title(f'SHAP Feature Importance - {model_name}')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'shap_plots/{model_name.lower().replace(\" \", \"_\")}_importance.png')\n",
    "            plt.close()\n",
    "            print(f\"  Feature importance plot saved to shap_plots/{model_name.lower().replace(' ', '_')}_importance.png\")\n",
    "            \n",
    "            # Dependence plots for top 3 features\n",
    "            # Get mean absolute SHAP values for each feature\n",
    "            feature_importance = np.abs(shap_values_to_plot).mean(0)\n",
    "            most_important_features = np.argsort(feature_importance)[-3:][::-1]\n",
    "            \n",
    "            for i, feature_idx in enumerate(most_important_features):\n",
    "                feature_name = feature_names[feature_idx]\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                shap.dependence_plot(feature_idx, shap_values_to_plot, X_val, feature_names=feature_names, show=False)\n",
    "                plt.title(f'SHAP Dependence Plot - {feature_name} ({model_name})')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'shap_plots/{model_name.lower().replace(\" \", \"_\")}_dependence_{i+1}.png')\n",
    "                plt.close()\n",
    "                print(f\"  Dependence plot for {feature_name} saved\")\n",
    "            \n",
    "            # Force plot for a single prediction (first positive case in validation set)\n",
    "            positive_indices = np.where(y_val == 1)[0]\n",
    "            if len(positive_indices) > 0:\n",
    "                idx = positive_indices[0]\n",
    "                plt.figure(figsize=(12, 3))\n",
    "                \n",
    "                if hasattr(model, 'feature_importances_'):\n",
    "                    # For tree-based models\n",
    "                    if isinstance(shap_values, list):\n",
    "                        # Get expected value for positive class\n",
    "                        expected_value = explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value\n",
    "                    else:\n",
    "                        expected_value = explainer.expected_value\n",
    "                    \n",
    "                    # Create force plot\n",
    "                    shap.force_plot(expected_value, shap_values_to_plot[idx], X_val.iloc[idx], feature_names=feature_names, matplotlib=True, show=False)\n",
    "                else:\n",
    "                    # For other models\n",
    "                    shap.force_plot(explainer.expected_value[1], shap_values[1][0], X_val.iloc[0], feature_names=feature_names, matplotlib=True, show=False)\n",
    "                \n",
    "                plt.title(f'SHAP Force Plot - Single Prediction ({model_name})')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'shap_plots/{model_name.lower().replace(\" \", \"_\")}_force_plot.png')\n",
    "                plt.close()\n",
    "                print(f\"  Force plot saved to shap_plots/{model_name.lower().replace(' ', '_')}_force_plot.png\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating SHAP explanations for {model_name}: {str(e)}\")\n",
    "    \n",
    "    print(\"\\nSHAP explanations completed!\")\n",
    "\n",
    "def save_best_model(results, performance_df):\n",
    "    \"\"\"\n",
    "    Save the best model based on F1 score\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Results for each model\n",
    "    performance_df : pd.DataFrame\n",
    "        Dataframe with model performance metrics\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 6: SAVING BEST MODEL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get the best model (first row in performance dataframe, sorted by F1 score)\n",
    "    best_model_name = performance_df.index[0]\n",
    "    best_model = results[best_model_name]['model']\n",
    "    \n",
    "    print(f\"\\nBest model: {best_model_name}\")\n",
    "    print(f\"Validation F1 Score: {performance_df.loc[best_model_name, 'Val F1']:.4f}\")\n",
    "    print(f\"Validation Recall: {performance_df.loc[best_model_name, 'Val Recall']:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    model_path = 'models/best_stroke_model.pkl'\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "    \n",
    "    print(f\"\\nBest model saved to {model_path}\")\n",
    "    \n",
    "    # Save model info\n",
    "    model_info = {\n",
    "        'model_name': best_model_name,\n",
    "        'metrics': {\n",
    "            'accuracy': performance_df.loc[best_model_name, 'Val Accuracy'],\n",
    "            'precision': performance_df.loc[best_model_name, 'Val Precision'],\n",
    "            'recall': performance_df.loc[best_model_name, 'Val Recall'],\n",
    "            'f1': performance_df.loc[best_model_name, 'Val F1'],\n",
    "            'roc_auc': performance_df.loc[best_model_name, 'Val ROC AUC']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    model_info_path = 'models/model_info.json'\n",
    "    with open(model_info_path, 'w') as f:\n",
    "        import json\n",
    "        json.dump(model_info, f, indent=4)\n",
    "    \n",
    "    print(f\"Model info saved to {model_info_path}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the stroke prediction pipeline\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STROKE PREDICTION PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Load and explore data\n",
    "    df = load_and_explore_data(\"healthcare-dataset-stroke-data.csv\")\n",
    "    \n",
    "    # Step 2: Preprocess data\n",
    "    X, y, scaler = preprocess_data(df)\n",
    "    \n",
    "    # Step 3: Handle class imbalance\n",
    "    X_balanced, y_balanced = handle_imbalance(X, y, method='smote')\n",
    "    \n",
    "    # Step 4: Train and evaluate models\n",
    "    results, performance_df, (X_val, y_val) = train_and_evaluate_models(X_balanced, y_balanced, cv=5)\n",
    "    \n",
    "    # Step 5: Create SHAP explanations\n",
    "    create_shap_explanations(results, X_val, X.columns)\n",
    "    \n",
    "    # Step 6: Save best model\n",
    "    save_best_model(results, performance_df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PIPELINE COMPLETED!\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
