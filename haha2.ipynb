{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stroke Prediction: Model Training & Evaluation\n",
    "\n",
    "This notebook focuses on training and evaluating predictive models for stroke prediction:\n",
    "1. Loading the preprocessed dataset\n",
    "2. Handling class imbalance with SMOTE\n",
    "3. Training multiple classification models\n",
    "4. Building a dense stacking ensemble model\n",
    "5. Evaluating models with robust metrics\n",
    "6. Saving the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Class imbalance handling\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, \n",
    "    AdaBoostClassifier, ExtraTreesClassifier,\n",
    "    VotingClassifier, StackingClassifier\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    precision_recall_curve, auc, average_precision_score, roc_curve\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Ignore warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Create directories for outputs\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "os.makedirs('figures/model_evaluation', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed and encoded dataset\n",
    "df = pd.read_csv('data/processed/stroke_dataset_encoded.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of features: {df.shape[1] - 1}\")\n",
    "print(f\"Target distribution:\\n{df['stroke'].value_counts(normalize=True) * 100}\")\n",
    "\n",
    "# Check for any missing values that might have been introduced\n",
    "missing_values = df.isnull().sum()\n",
    "if missing_values.sum() > 0:\n",
    "    print(\"\\nMissing values:\")\n",
    "    print(missing_values[missing_values > 0])\n",
    "else:\n",
    "    print(\"\\nNo missing values found.\")\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "X = df.drop('stroke', axis=1)\n",
    "y = df['stroke']\n",
    "\n",
    "# Split the data into training and testing sets with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "# Further split training data to create a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=SEED, stratify=y_train  # 0.25 * 0.8 = 0.2 of original data\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "# Check class distribution in the splits\n",
    "def print_class_distribution(name, y_data):\n",
    "    counts = np.bincount(y_data)\n",
    "    percent = counts / len(y_data) * 100\n",
    "    percent_str = [f\"{p:.2f}%\" for p in percent]\n",
    "    print(f\"{name} set: {counts.tolist()} ({percent_str})\")\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "print_class_distribution(\"Training\", y_train)\n",
    "print_class_distribution(\"Validation\", y_val)\n",
    "print_class_distribution(\"Test\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handle Class Imbalance with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply SMOTE to handle class imbalance\n",
    "print(\"Applying SMOTE to balance the training set...\")\n",
    "smote = SMOTE(random_state=SEED, sampling_strategy='auto')\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Shape after SMOTE: {X_train_smote.shape}\")\n",
    "print(f\"Class distribution after SMOTE: {np.bincount(y_train_smote)} ({np.bincount(y_train_smote) / len(y_train_smote) * 100:.2f}%)\")\n",
    "\n",
    "# Also try BorderlineSMOTE for comparison\n",
    "print(\"\\nApplying BorderlineSMOTE to balance the training set...\")\n",
    "bsmote = BorderlineSMOTE(random_state=SEED, kind='borderline-1')\n",
    "X_train_bsmote, y_train_bsmote = bsmote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Shape after BorderlineSMOTE: {X_train_bsmote.shape}\")\n",
    "print(f\"Class distribution after BorderlineSMOTE: {np.bincount(y_train_bsmote)} ({np.bincount(y_train_bsmote) / len(y_train_bsmote) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the balanced vs. unbalanced class distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Original distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.countplot(x=y_train, palette='Set2')\n",
    "plt.title('Original Class Distribution')\n",
    "plt.xlabel('Stroke')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# SMOTE distribution\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.countplot(x=y_train_smote, palette='Set2')\n",
    "plt.title('After SMOTE')\n",
    "plt.xlabel('Stroke')\n",
    "\n",
    "# BorderlineSMOTE distribution\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.countplot(x=y_train_bsmote, palette='Set2')\n",
    "plt.title('After BorderlineSMOTE')\n",
    "plt.xlabel('Stroke')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/model_evaluation/class_balance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Evaluation Metrics and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_val, y_val, model_name, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate a model on the validation set and return metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : classifier object\n",
    "        The trained model to evaluate\n",
    "    X_val : array-like\n",
    "        Validation features\n",
    "    y_val : array-like\n",
    "        Validation target\n",
    "    model_name : str\n",
    "        Name of the model for logging\n",
    "    threshold : float, default=0.5\n",
    "        Classification threshold\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    try:\n",
    "        y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    except:\n",
    "        # Some models might not have predict_proba\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_pred_proba = None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "    metrics['accuracy'] = accuracy_score(y_val, y_pred)\n",
    "    metrics['precision'] = precision_score(y_val, y_pred)\n",
    "    metrics['recall'] = recall_score(y_val, y_pred)\n",
    "    metrics['f1'] = f1_score(y_val, y_pred)\n",
    "    \n",
    "    if y_pred_proba is not None:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_val, y_pred_proba)\n",
    "        metrics['average_precision'] = average_precision_score(y_val, y_pred_proba)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    metrics['confusion_matrix'] = confusion_matrix(y_val, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} Evaluation:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "    if 'roc_auc' in metrics:\n",
    "        print(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "        print(f\"Average Precision: {metrics['average_precision']:.4f}\")\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(metrics['confusion_matrix'])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_roc_curve(models, X_val, y_val, figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Plot ROC curves for multiple models.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : dict\n",
    "        Dictionary of {model_name: model}\n",
    "    X_val : array-like\n",
    "        Validation features\n",
    "    y_val : array-like\n",
    "        Validation target\n",
    "    figsize : tuple, default=(10, 8)\n",
    "        Figure size\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "            fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.4f})')\n",
    "        except:\n",
    "            print(f\"{name} does not support predict_proba, skipping ROC curve.\")\n",
    "    \n",
    "    # Random classifier line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=15)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('figures/model_evaluation/roc_curve_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall_curve(models, X_val, y_val, figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Plot precision-recall curves for multiple models.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : dict\n",
    "        Dictionary of {model_name: model}\n",
    "    X_val : array-like\n",
    "        Validation features\n",
    "    y_val : array-like\n",
    "        Validation target\n",
    "    figsize : tuple, default=(10, 8)\n",
    "        Figure size\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Calculate baseline (no skill classifier)\n",
    "    no_skill = len(y_val[y_val == 1]) / len(y_val)\n",
    "    plt.plot([0, 1], [no_skill, no_skill], 'k--', lw=2, label='No Skill')\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "            precision, recall, _ = precision_recall_curve(y_val, y_pred_proba)\n",
    "            pr_auc = auc(recall, precision)\n",
    "            avg_precision = average_precision_score(y_val, y_pred_proba)\n",
    "            plt.plot(recall, precision, lw=2, \n",
    "                    label=f'{name} (AP = {avg_precision:.4f}, AUC = {pr_auc:.4f})')\n",
    "        except:\n",
    "            print(f\"{name} does not support predict_proba, skipping PR curve.\")\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall', fontsize=12)\n",
    "    plt.ylabel('Precision', fontsize=12)\n",
    "    plt.title('Precision-Recall Curve', fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('figures/model_evaluation/precision_recall_curve_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(cm, model_name, figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cm : array-like\n",
    "        Confusion matrix\n",
    "    model_name : str\n",
    "        Name of the model\n",
    "    figsize : tuple, default=(8, 6)\n",
    "        Figure size\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['No Stroke', 'Stroke'],\n",
    "                yticklabels=['No Stroke', 'Stroke'])\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.title(f'Confusion Matrix - {model_name}', fontsize=15)\n",
    "    plt.savefig(f'figures/model_evaluation/confusion_matrix_{model_name.replace(\" \", \"_\").lower()}.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importance(model, feature_names, model_name, figsize=(12, 10), top_n=20):\n",
    "    \"\"\"\n",
    "    Plot feature importance for a model, if available.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : classifier object\n",
    "        The trained model\n",
    "    feature_names : array-like\n",
    "        Names of the features\n",
    "    model_name : str\n",
    "        Name of the model\n",
    "    figsize : tuple, default=(12, 10)\n",
    "        Figure size\n",
    "    top_n : int, default=20\n",
    "        Number of top features to display\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            # For tree-based models\n",
    "            importances = model.feature_importances_\n",
    "        elif hasattr(model, 'coef_'):\n",
    "            # For linear models\n",
    "            importances = np.abs(model.coef_[0])\n",
    "        else:\n",
    "            print(f\"{model_name} does not have feature importances.\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame of feature importances\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "        })\n",
    "        \n",
    "        # Sort by importance\n",
    "        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot top N features\n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_importance.head(top_n))\n",
    "        plt.title(f'Top {top_n} Feature Importance - {model_name}', fontsize=15)\n",
    "        plt.xlabel('Importance', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'figures/model_evaluation/feature_importance_{model_name.replace(\" \", \"_\").lower()}.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return feature_importance\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting feature importance for {model_name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models for training\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=SEED),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=SEED),\n",
    "    'XGBoost': xgb.XGBClassifier(scale_pos_weight=10, random_state=SEED),\n",
    "    'LightGBM': lgb.LGBMClassifier(class_weight='balanced', random_state=SEED),\n",
    "    'CatBoost': cb.CatBoostClassifier(random_state=SEED, verbose=0),\n",
    "    'SVM': SVC(probability=True, class_weight='balanced', random_state=SEED),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5, weights='distance'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=SEED),\n",
    "    'Extra Trees': ExtraTreesClassifier(n_estimators=100, class_weight='balanced', random_state=SEED),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=SEED)\n",
    "}\n",
    "\n",
    "# Dictionary to store trained models and their metrics\n",
    "trained_models = {}\n",
    "model_metrics = {}\n",
    "\n",
    "# Train each model with SMOTE-balanced data\n",
    "print(\"Training base models with SMOTE...\")\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    metrics = evaluate_model(model, X_val_scaled, y_val, name)\n",
    "    \n",
    "    # Calculate training time\n",
    "    train_time = time.time() - start_time\n",
    "    metrics['train_time'] = train_time\n",
    "    print(f\"Training time: {train_time:.2f} seconds\")\n",
    "    \n",
    "    # Store trained model and metrics\n",
    "    trained_models[name] = model\n",
    "    model_metrics[name] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with BorderlineSMOTE for comparison\n",
    "trained_models_bsmote = {}\n",
    "model_metrics_bsmote = {}\n",
    "\n",
    "print(\"Training base models with BorderlineSMOTE...\")\n",
    "for name, model in models.items():\n",
    "    # Only train top performing models from SMOTE\n",
    "    if name not in ['Logistic Regression', 'Random Forest', 'XGBoost', 'CatBoost', 'LightGBM']:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nTraining {name} with BorderlineSMOTE...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create a fresh instance of the model\n",
    "    if name == 'Logistic Regression':\n",
    "        model = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=SEED)\n",
    "    elif name == 'Random Forest':\n",
    "        model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=SEED)\n",
    "    elif name == 'XGBoost':\n",
    "        model = xgb.XGBClassifier(scale_pos_weight=10, random_state=SEED)\n",
    "    elif name == 'LightGBM':\n",
    "        model = lgb.LGBMClassifier(class_weight='balanced', random_state=SEED)\n",
    "    elif name == 'CatBoost':\n",
    "        model = cb.CatBoostClassifier(random_state=SEED, verbose=0)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_bsmote, y_train_bsmote)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    metrics = evaluate_model(model, X_val_scaled, y_val, f\"{name} (BorderlineSMOTE)\")\n",
    "    \n",
    "    # Calculate training time\n",
    "    train_time = time.time() - start_time\n",
    "    metrics['train_time'] = train_time\n",
    "    print(f\"Training time: {train_time:.2f} seconds\")\n",
    "    \n",
    "    # Store trained model and metrics\n",
    "    trained_models_bsmote[name] = model\n",
    "    model_metrics_bsmote[name] = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Base Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with all model metrics\n",
    "metrics_df = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC', 'Avg Precision', 'Training Time'])\n",
    "\n",
    "# Add metrics for SMOTE models\n",
    "for name, metrics in model_metrics.items():\n",
    "    metrics_df = pd.concat([metrics_df, pd.DataFrame([{\n",
    "        'Model': name,\n",
    "        'Sampling': 'SMOTE',\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1 Score': metrics['f1'],\n",
    "        'ROC AUC': metrics.get('roc_auc', np.nan),\n",
    "        'Avg Precision': metrics.get('average_precision', np.nan),\n",
    "        'Training Time': metrics['train_time']\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "# Add metrics for BorderlineSMOTE models\n",
    "for name, metrics in model_metrics_bsmote.items():\n",
    "    metrics_df = pd.concat([metrics_df, pd.DataFrame([{\n",
    "        'Model': name,\n",
    "        'Sampling': 'BorderlineSMOTE',\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1 Score': metrics['f1'],\n",
    "        'ROC AUC': metrics.get('roc_auc', np.nan),\n",
    "        'Avg Precision': metrics.get('average_precision', np.nan),\n",
    "        'Training Time': metrics['train_time']\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "# Sort by F1 score (descending)\n",
    "metrics_df = metrics_df.sort_values('F1 Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display all metrics\n",
    "print(\"Model Performance Comparison (Sorted by F1 Score):\")\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance comparison\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot F1 score comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.barplot(x='F1 Score', y='Model', hue='Sampling', data=metrics_df)\n",
    "plt.title('F1 Score Comparison', fontsize=14)\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot ROC AUC comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.barplot(x='ROC AUC', y='Model', hue='Sampling', data=metrics_df)\n",
    "plt.title('ROC AUC Comparison', fontsize=14)\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot Precision comparison\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.barplot(x='Precision', y='Model', hue='Sampling', data=metrics_df)\n",
    "plt.title('Precision Comparison', fontsize=14)\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot Recall comparison\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.barplot(x='Recall', y='Model', hue='Sampling', data=metrics_df)\n",
    "plt.title('Recall Comparison', fontsize=14)\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/model_evaluation/model_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for the top models\n",
    "top_models = {}\n",
    "for name in metrics_df['Model'].unique()[:5]:  # Get top 5 unique model names\n",
    "    # Get the better version (SMOTE or BorderlineSMOTE) based on F1 score\n",
    "    model_metrics_filtered = metrics_df[metrics_df['Model'] == name]\n",
    "    best_sampling = model_metrics_filtered.iloc[0]['Sampling']\n",
    "    \n",
    "    if best_sampling == 'SMOTE':\n",
    "        top_models[name] = trained_models[name]\n",
    "    else:\n",
    "        top_models[name] = trained_models_bsmote[name]\n",
    "\n",
    "# Plot ROC curves\n",
    "plot_roc_curve(top_models, X_val_scaled, y_val)\n",
    "\n",
    "# Plot precision-recall curves\n",
    "plot_precision_recall_curve(top_models, X_val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for top 3 models\n",
    "for name, model in list(top_models.items())[:3]:\n",
    "    # Get confusion matrix\n",
    "    y_pred = model.predict(X_val_scaled)\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    \n",
    "    # Plot\n",
    "    plot_confusion_matrix(cm, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance for the best performing models\n",
    "feature_importances = {}\n",
    "\n",
    "for name, model in list(top_models.items())[:3]:\n",
    "    importance = plot_feature_importance(model, X_train.columns, name)\n",
    "    if importance is not None:\n",
    "        feature_importances[name] = importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Tuning for Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best 3 models for hyperparameter tuning\n",
    "best_models = metrics_df['Model'].unique()[:3]\n",
    "print(f\"Best models for tuning: {best_models}\")\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {}\n",
    "\n",
    "# Define parameter grids based on best models\n",
    "for model_name in best_models:\n",
    "    if model_name == 'XGBoost':\n",
    "        param_grids[model_name] = {\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'min_child_weight': [1, 3, 5],\n",
    "            'subsample': [0.7, 0.8, 0.9],\n",
    "            'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "            'n_estimators': [100, 200],\n",
    "            'scale_pos_weight': [1, 5, 10]\n",
    "        }\n",
    "    elif model_name == 'Random Forest':\n",
    "        param_grids[model_name] = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'class_weight': ['balanced']\n",
    "        }\n",
    "    elif model_name == 'LightGBM':\n",
    "        param_grids[model_name] = {\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'num_leaves': [31, 63, 127],\n",
    "            'max_depth': [-1, 5, 10],\n",
    "            'min_child_samples': [5, 10, 20],\n",
    "            'subsample': [0.7, 0.8, 0.9],\n",
    "            'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "            'n_estimators': [100, 200],\n",
    "            'class_weight': ['balanced']\n",
    "        }\n",
    "    elif model_name == 'CatBoost':\n",
    "        param_grids[model_name] = {\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'depth': [4, 6, 8],\n",
    "            'l2_leaf_reg': [1, 3, 5],\n",
    "            'iterations': [100, 200],\n",
    "            'verbose': [0]\n",
    "        }\n",
    "    elif model_name == 'Gradient Boosting':\n",
    "        param_grids[model_name] = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'subsample': [0.7, 0.8, 0.9]\n",
    "        }\n",
    "    elif model_name == 'Logistic Regression':\n",
    "        param_grids[model_name] = {\n",
    "            'C': [0.01, 0.1, 1.0, 10.0],\n",
    "            'solver': ['liblinear', 'saga'],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'class_weight': ['balanced'],\n",
    "            'max_iter': [1000]\n",
    "        }\n",
    "    else:\n",
    "        print(f\"No parameter grid defined for {model_name}, skipping.\")\n",
    "        continue\n",
    "\n",
    "# Dictionary to store tuned models\n",
    "tuned_models = {}\n",
    "tuned_model_metrics = {}\n",
    "\n",
    "# Perform grid search for each model\n",
    "for model_name in best_models:\n",
    "    if model_name not in param_grids:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nPerforming hyperparameter tuning for {model_name}...\")\n",
    "    \n",
    "    # Get the base model\n",
    "    if model_name == 'XGBoost':\n",
    "        base_model = xgb.XGBClassifier(random_state=SEED)\n",
    "    elif model_name == 'Random Forest':\n",
    "        base_model = RandomForestClassifier(random_state=SEED)\n",
    "    elif model_name == 'LightGBM':\n",
    "        base_model = lgb.LGBMClassifier(random_state=SEED)\n",
    "    elif model_name == 'CatBoost':\n",
    "        base_model = cb.CatBoostClassifier(random_state=SEED)\n",
    "    elif model_name == 'Gradient Boosting':\n",
    "        base_model = GradientBoostingClassifier(random_state=SEED)\n",
    "    elif model_name == 'Logistic Regression':\n",
    "        base_model = LogisticRegression(random_state=SEED)\n",
    "    \n",
    "    # Set up grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_grid=param_grids[model_name],\n",
    "        cv=3,  # Use a smaller CV to speed up the search\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit grid search\n",
    "    grid_search.fit(X_train_smote, y_train_smote)\n",
    "    \n",
    "    # Get best estimator and parameters\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate best model on validation set\n",
    "    metrics = evaluate_model(best_model, X_val_scaled, y_val, f\"{model_name} (Tuned)\")\n",
    "    \n",
    "    # Store tuned model and metrics\n",
    "    tuned_models[model_name] = best_model\n",
    "    tuned_model_metrics[model_name] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tuned models with base models\n",
    "compare_df = pd.DataFrame(columns=['Model', 'Version', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC', 'Avg Precision'])\n",
    "\n",
    "# Add base model metrics\n",
    "for name in tuned_models.keys():\n",
    "    # Find the best version (SMOTE or BorderlineSMOTE)\n",
    "    model_metrics_filtered = metrics_df[metrics_df['Model'] == name]\n",
    "    best_sampling = model_metrics_filtered.iloc[0]['Sampling']\n",
    "    metrics = model_metrics_filtered.iloc[0]\n",
    "    \n",
    "    compare_df = pd.concat([compare_df, pd.DataFrame([{\n",
    "        'Model': name,\n",
    "        'Version': f'Base ({best_sampling})',\n",
    "        'Accuracy': metrics['Accuracy'],\n",
    "        'Precision': metrics['Precision'],\n",
    "        'Recall': metrics['Recall'],\n",
    "        'F1 Score': metrics['F1 Score'],\n",
    "        'ROC AUC': metrics['ROC AUC'],\n",
    "        'Avg Precision': metrics['Avg Precision']\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "# Add tuned model metrics\n",
    "for name, metrics in tuned_model_metrics.items():\n",
    "    compare_df = pd.concat([compare_df, pd.DataFrame([{\n",
    "        'Model': name,\n",
    "        'Version': 'Tuned',\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1 Score': metrics['f1'],\n",
    "        'ROC AUC': metrics.get('roc_auc', np.nan),\n",
    "        'Avg Precision': metrics.get('average_precision', np.nan)\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "# Sort by model and then version\n",
    "compare_df = compare_df.sort_values(['Model', 'Version']).reset_index(drop=True)\n",
    "\n",
    "# Display comparison\n",
    "print(\"Base vs. Tuned Models Performance Comparison:\")\n",
    "compare_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize base vs. tuned model performance comparison\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot F1 score comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.barplot(x='F1 Score', y='Model', hue='Version', data=compare_df)\n",
    "plt.title('F1 Score: Base vs. Tuned Models', fontsize=14)\n",
    "plt.xlim(0.5, 1)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot ROC AUC comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.barplot(x='ROC AUC', y='Model', hue='Version', data=compare_df)\n",
    "plt.title('ROC AUC: Base vs. Tuned Models', fontsize=14)\n",
    "plt.xlim(0.5, 1)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot Precision comparison\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.barplot(x='Precision', y='Model', hue='Version', data=compare_df)\n",
    "plt.title('Precision: Base vs. Tuned Models', fontsize=14)\n",
    "plt.xlim(0.5, 1)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot Recall comparison\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.barplot(x='Recall', y='Model', hue='Version', data=compare_df)\n",
    "plt.title('Recall: Base vs. Tuned Models', fontsize=14)\n",
    "plt.xlim(0.5, 1)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/model_evaluation/tuned_vs_base_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Build Dense Stacking Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dense Stacking Ensemble model using the best tuned models\n",
    "print(\"Building Dense Stacking Ensemble Model...\")\n",
    "\n",
    "# Select base models for the ensemble (use tuned models)\n",
    "base_models = [\n",
    "    (name, model) for name, model in tuned_models.items()\n",
    "]\n",
    "\n",
    "# Add some additional base models for diversity\n",
    "if 'AdaBoost' not in tuned_models and 'AdaBoost' in trained_models:\n",
    "    base_models.append(('AdaBoost', trained_models['AdaBoost']))\n",
    "if 'Extra Trees' not in tuned_models and 'Extra Trees' in trained_models:\n",
    "    base_models.append(('Extra Trees', trained_models['Extra Trees']))\n",
    "\n",
    "print(f\"Base models for stacking: {[name for name, _ in base_models]}\")\n",
    "\n",
    "# Define meta-learner (final estimator)\n",
    "meta_learner = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=SEED)\n",
    "\n",
    "# Create the stacking ensemble\n",
    "stacking_ensemble = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5,\n",
    "    stack_method='predict_proba',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the stacking ensemble\n",
    "print(\"Training Stacking Ensemble...\")\n",
    "start_time = time.time()\n",
    "stacking_ensemble.fit(X_train_smote, y_train_smote)\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate the stacking ensemble\n",
    "stacking_metrics = evaluate_model(stacking_ensemble, X_val_scaled, y_val, \"Dense Stacking Ensemble\")\n",
    "stacking_metrics['train_time'] = training_time\n",
    "\n",
    "# Create weighted voting ensemble as an alternative\n",
    "print(\"\\nBuilding Weighted Voting Ensemble...\")\n",
    "\n",
    "# Define weights based on validation F1 scores\n",
    "voting_weights = []\n",
    "for name, _ in base_models:\n",
    "    if name in tuned_model_metrics:\n",
    "        voting_weights.append(tuned_model_metrics[name]['f1'])\n",
    "    else:\n",
    "        voting_weights.append(model_metrics[name]['f1'])\n",
    "\n",
    "print(f\"Voting weights: {voting_weights}\")\n",
    "\n",
    "# Create the voting ensemble\n",
    "voting_ensemble = VotingClassifier(\n",
    "    estimators=base_models,\n",
    "    voting='soft',\n",
    "    weights=voting_weights,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the voting ensemble\n",
    "print(\"Training Voting Ensemble...\")\n",
    "start_time = time.time()\n",
    "voting_ensemble.fit(X_train_smote, y_train_smote)\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate the voting ensemble\n",
    "voting_metrics = evaluate_model(voting_ensemble, X_val_scaled, y_val, \"Weighted Voting Ensemble\")\n",
    "voting_metrics['train_time'] = training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ensemble models with the best individual models\n",
    "ensemble_comparison = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC', 'Avg Precision'])\n",
    "\n",
    "# Add best individual model (tuned)\n",
    "best_tuned_model = None\n",
    "best_f1 = 0\n",
    "\n",
    "for name, metrics in tuned_model_metrics.items():\n",
    "    if metrics['f1'] > best_f1:\n",
    "        best_f1 = metrics['f1']\n",
    "        best_tuned_model = name\n",
    "\n",
    "best_metrics = tuned_model_metrics[best_tuned_model]\n",
    "ensemble_comparison = pd.concat([ensemble_comparison, pd.DataFrame([{\n",
    "    'Model': f\"Best Individual ({best_tuned_model})\",\n",
    "    'Accuracy': best_metrics['accuracy'],\n",
    "    'Precision': best_metrics['precision'],\n",
    "    'Recall': best_metrics['recall'],\n",
    "    'F1 Score': best_metrics['f1'],\n",
    "    'ROC AUC': best_metrics.get('roc_auc', np.nan),\n",
    "    'Avg Precision': best_metrics.get('average_precision', np.nan)\n",
    "}])], ignore_index=True)\n",
    "\n",
    "# Add stacking ensemble metrics\n",
    "ensemble_comparison = pd.concat([ensemble_comparison, pd.DataFrame([{\n",
    "    'Model': \"Dense Stacking Ensemble\",\n",
    "    'Accuracy': stacking_metrics['accuracy'],\n",
    "    'Precision': stacking_metrics['precision'],\n",
    "    'Recall': stacking_metrics['recall'],\n",
    "    'F1 Score': stacking_metrics['f1'],\n",
    "    'ROC AUC': stacking_metrics.get('roc_auc', np.nan),\n",
    "    'Avg Precision': stacking_metrics.get('average_precision', np.nan)\n",
    "}])], ignore_index=True)\n",
    "\n",
    "# Add voting ensemble metrics\n",
    "ensemble_comparison = pd.concat([ensemble_comparison, pd.DataFrame([{\n",
    "    'Model': \"Weighted Voting Ensemble\",\n",
    "    'Accuracy': voting_metrics['accuracy'],\n",
    "    'Precision': voting_metrics['precision'],\n",
    "    'Recall': voting_metrics['recall'],\n",
    "    'F1 Score': voting_metrics['f1'],\n",
    "    'ROC AUC': voting_metrics.get('roc_auc', np.nan),\n",
    "    'Avg Precision': voting_metrics.get('average_precision', np.nan)\n",
    "}])], ignore_index=True)\n",
    "\n",
    "# Sort by F1 score\n",
    "ensemble_comparison = ensemble_comparison.sort_values('F1 Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display comparison\n",
    "print(\"Ensemble vs. Best Individual Model Performance Comparison:\")\n",
    "ensemble_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ensemble comparison\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "metrics_to_plot = ['F1 Score', 'ROC AUC', 'Precision', 'Recall']\n",
    "for i, metric in enumerate(metrics_to_plot, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.barplot(x=metric, y='Model', data=ensemble_comparison, palette='viridis')\n",
    "    plt.title(f'{metric} Comparison', fontsize=14)\n",
    "    plt.xlim(0.5, 1)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/model_evaluation/ensemble_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ROC curves\n",
    "ensemble_models = {\n",
    "    'Dense Stacking Ensemble': stacking_ensemble,\n",
    "    'Weighted Voting Ensemble': voting_ensemble,\n",
    "    f'Best Individual ({best_tuned_model})': tuned_models[best_tuned_model]\n",
    "}\n",
    "\n",
    "# Plot ROC curves\n",
    "plot_roc_curve(ensemble_models, X_val_scaled, y_val)\n",
    "\n",
    "# Plot precision-recall curves\n",
    "plot_precision_recall_curve(ensemble_models, X_val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model based on validation performance\n",
    "best_model_name = ensemble_comparison.iloc[0]['Model']\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "\n",
    "# Get the corresponding model object\n",
    "if best_model_name == \"Dense Stacking Ensemble\":\n",
    "    best_model = stacking_ensemble\n",
    "elif best_model_name == \"Weighted Voting Ensemble\":\n",
    "    best_model = voting_ensemble\n",
    "else:\n",
    "    # Extract model name from \"Best Individual (ModelName)\"\n",
    "    model_name = best_model_name.split('(')[1].split(')')[0]\n",
    "    best_model = tuned_models[model_name]\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "print(\"\\nEvaluating best model on test set...\")\n",
    "test_metrics = evaluate_model(best_model, X_test_scaled, y_test, f\"Best Model ({best_model_name}) - Test Set\")\n",
    "\n",
    "# Plot confusion matrix for test set\n",
    "y_test_pred = best_model.predict(X_test_scaled)\n",
    "test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "plot_confusion_matrix(test_cm, f\"Best Model ({best_model_name}) - Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed classification report\n",
    "print(\"Detailed Classification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Calculate ROC curve and AUC for test set\n",
    "try:\n",
    "    y_test_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_test_proba)\n",
    "    test_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'ROC curve (AUC = {test_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title(f'ROC Curve - Test Set', fontsize=15)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('figures/model_evaluation/test_set_roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Precision-Recall curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_test_proba)\n",
    "    test_avg_precision = average_precision_score(y_test, y_test_proba)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    no_skill = len(y_test[y_test == 1]) / len(y_test)\n",
    "    plt.plot([0, 1], [no_skill, no_skill], 'k--', lw=2, label='No Skill')\n",
    "    plt.plot(recall, precision, lw=2, label=f'PR curve (AP = {test_avg_precision:.4f})')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall', fontsize=12)\n",
    "    plt.ylabel('Precision', fontsize=12)\n",
    "    plt.title(f'Precision-Recall Curve - Test Set', fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('figures/model_evaluation/test_set_pr_curve.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error generating probability-based curves: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model, scaler, and metadata\n",
    "print(f\"Saving best model: {best_model_name}\")\n",
    "\n",
    "# Save model\n",
    "model_filename = 'models/stroke_prediction_model.pkl'\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f\"Model saved to {model_filename}\")\n",
    "\n",
    "# Save scaler\n",
    "scaler_filename = 'models/stroke_prediction_scaler.pkl'\n",
    "with open(scaler_filename, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"Scaler saved to {scaler_filename}\")\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'feature_names': list(X.columns),\n",
    "    'num_features': len(X.columns),\n",
    "    'test_metrics': {\n",
    "        'accuracy': test_metrics['accuracy'],\n",
    "        'precision': test_metrics['precision'],\n",
    "        'recall': test_metrics['recall'],\n",
    "        'f1': test_metrics['f1'],\n",
    "        'roc_auc': test_metrics.get('roc_auc', None),\n",
    "        'avg_precision': test_metrics.get('average_precision', None)\n",
    "    },\n",
    "    'class_distribution': {\n",
    "        'negative': int(np.sum(y == 0)),\n",
    "        'positive': int(np.sum(y == 1))\n",
    "    },\n",
    "    'timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "metadata_filename = 'models/stroke_prediction_metadata.json'\n",
    "with open(metadata_filename, 'w') as f:\n",
    "    import json\n",
    "    json.dump(model_metadata, f, indent=4)\n",
    "print(f\"Model metadata saved to {metadata_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a helper function for making predictions with the saved model\n",
    "def load_model_and_predict(input_data, model_path='models/stroke_prediction_model.pkl', \n",
    "                           scaler_path='models/stroke_prediction_scaler.pkl'):\n",
    "    \"\"\"\n",
    "    Load the saved model and make predictions on new data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_data : pd.DataFrame\n",
    "        Input data with the same features as the training data\n",
    "    model_path : str\n",
    "        Path to the saved model\n",
    "    scaler_path : str\n",
    "        Path to the saved scaler\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Predicted class and probability\n",
    "    \"\"\"\n",
    "    # Load model and scaler\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    with open(scaler_path, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    \n",
    "    # Scale the input data\n",
    "    input_scaled = scaler.transform(input_data)\n",
    "    \n",
    "    # Make predictions\n",
    "    try:\n",
    "        pred_proba = model.predict_proba(input_scaled)[:, 1]\n",
    "        pred_class = model.predict(input_scaled)\n",
    "        return pred_class, pred_proba\n",
    "    except:\n",
    "        pred_class = model.predict(input_scaled)\n",
    "        return pred_class, None\n",
    "\n",
    "# Test the function with a sample from the test set\n",
    "sample_idx = np.random.randint(0, len(X_test))\n",
    "sample_input = X_test.iloc[[sample_idx]]\n",
    "true_label = y_test.iloc[sample_idx]\n",
    "\n",
    "print(f\"Testing prediction function with sample {sample_idx}...\")\n",
    "print(f\"True label: {true_label}\")\n",
    "\n",
    "# Make prediction with the saved model\n",
    "pred_class, pred_proba = load_model_and_predict(sample_input)\n",
    "\n",
    "print(f\"Predicted class: {pred_class[0]}\")\n",
    "if pred_proba is not None:\n",
    "    print(f\"Predicted probability: {pred_proba[0]:.4f}\")\n",
    "\n",
    "print(\"\\nModel training and evaluation completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Model Training Process\n",
    "\n",
    "In this notebook, we've developed a comprehensive stroke prediction model using the following approach:\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - Loaded preprocessed dataset with encoded features\n",
    "   - Split data into training, validation, and test sets\n",
    "   - Standardized numerical features\n",
    "\n",
    "2. **Class Imbalance Handling**:\n",
    "   - Applied SMOTE and BorderlineSMOTE to balance the training set\n",
    "   - Compared performance of both approaches\n",
    "\n",
    "3. **Base Model Training**:\n",
    "   - Trained multiple classification models including Logistic Regression, Random Forest, XGBoost, LightGBM, etc.\n",
    "   - Evaluated models on validation set using multiple metrics\n",
    "\n",
    "4. **Hyperparameter Tuning**:\n",
    "   - Applied grid search to tune the best performing models\n",
    "   - Compared tuned models with base models\n",
    "\n",
    "5. **Ensemble Learning**:\n",
    "   - Built a Dense Stacking Ensemble using the best tuned models\n",
    "   - Created a Weighted Voting Ensemble as an alternative\n",
    "   - Compared ensemble models with individual models\n",
    "\n",
    "6. **Final Evaluation**:\n",
    "   - Evaluated the best model on the test set\n",
    "   - Generated detailed metrics and visualizations\n",
    "\n",
    "7. **Model Saving**:\n",
    "   - Saved the best model, scaler, and metadata for future use\n",
    "   - Created a helper function for making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Enhanced Stroke Prediction Model\n",
    "# This script implements advanced techniques to achieve high performance for stroke prediction\n",
    "\n",
    "# General libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel, RFECV\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Class imbalance handling\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier, RUSBoostClassifier, EasyEnsembleClassifier\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, \n",
    "    AdaBoostClassifier, ExtraTreesClassifier,\n",
    "    VotingClassifier, StackingClassifier, BaggingClassifier\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# For XGBoost, LightGBM, and CatBoost\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    precision_recall_curve, auc, average_precision_score, roc_curve,\n",
    "    balanced_accuracy_score, cohen_kappa_score, matthews_corrcoef\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Ignore warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Create directories for outputs\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "os.makedirs('figures/model_evaluation', exist_ok=True)\n",
    "\n",
    "# Function to evaluate model with multiple metrics\n",
    "def evaluate_model_comprehensive(model, X_val, y_val, model_name, threshold=0.5, print_report=True):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of a model with multiple metrics.\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    try:\n",
    "        y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    except:\n",
    "        # Some models might not have predict_proba\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_pred_proba = None\n",
    "    \n",
    "    # Calculate standard metrics\n",
    "    metrics = {}\n",
    "    metrics['accuracy'] = accuracy_score(y_val, y_pred)\n",
    "    metrics['balanced_accuracy'] = balanced_accuracy_score(y_val, y_pred)\n",
    "    metrics['precision'] = precision_score(y_val, y_pred)\n",
    "    metrics['recall'] = recall_score(y_val, y_pred)\n",
    "    metrics['f1'] = f1_score(y_val, y_pred)\n",
    "    metrics['cohen_kappa'] = cohen_kappa_score(y_val, y_pred)\n",
    "    metrics['matthews_corrcoef'] = matthews_corrcoef(y_val, y_pred)\n",
    "    \n",
    "    if y_pred_proba is not None:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_val, y_pred_proba)\n",
    "        metrics['average_precision'] = average_precision_score(y_val, y_pred_proba)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    metrics['confusion_matrix'] = cm\n",
    "    \n",
    "    # Additional metrics derived from confusion matrix\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    metrics['npv'] = tn / (tn + fn) if (tn + fn) > 0 else 0  # Negative Predictive Value\n",
    "    \n",
    "    # Calculate G-mean\n",
    "    metrics['g_mean'] = np.sqrt(metrics['recall'] * metrics['specificity'])\n",
    "    \n",
    "    # Print results if requested\n",
    "    if print_report:\n",
    "        print(f\"\\n{model_name} Evaluation (threshold={threshold:.2f}):\")\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"Balanced Accuracy: {metrics['balanced_accuracy']:.4f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall/Sensitivity: {metrics['recall']:.4f}\")\n",
    "        print(f\"Specificity: {metrics['specificity']:.4f}\")\n",
    "        print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"Cohen's Kappa: {metrics['cohen_kappa']:.4f}\")\n",
    "        print(f\"Matthews Correlation Coefficient: {metrics['matthews_corrcoef']:.4f}\")\n",
    "        print(f\"G-mean: {metrics['g_mean']:.4f}\")\n",
    "        \n",
    "        if 'roc_auc' in metrics:\n",
    "            print(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "            print(f\"Average Precision: {metrics['average_precision']:.4f}\")\n",
    "        \n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_val, y_pred))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Function for finding optimal threshold\n",
    "def find_optimal_threshold(model, X_val, y_val, metric='f1', thresholds=None):\n",
    "    \"\"\"Find the optimal threshold to maximize a given metric.\"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.01, 0.99, 99)\n",
    "        \n",
    "    try:\n",
    "        y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    except:\n",
    "        print(\"Model doesn't support predict_proba. Using default threshold of 0.5.\")\n",
    "        return 0.5\n",
    "    \n",
    "    scores = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        \n",
    "        if metric == 'f1':\n",
    "            score = f1_score(y_val, y_pred)\n",
    "        elif metric == 'recall':\n",
    "            score = recall_score(y_val, y_pred)\n",
    "        elif metric == 'precision':\n",
    "            score = precision_score(y_val, y_pred)\n",
    "        elif metric == 'g_mean':\n",
    "            recall = recall_score(y_val, y_pred)\n",
    "            tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            score = np.sqrt(recall * specificity)\n",
    "        elif metric == 'balanced_accuracy':\n",
    "            score = balanced_accuracy_score(y_val, y_pred)\n",
    "        else:\n",
    "            score = f1_score(y_val, y_pred)  # Default to F1\n",
    "            \n",
    "        scores.append(score)\n",
    "    \n",
    "    best_idx = np.argmax(scores)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    best_score = scores[best_idx]\n",
    "    \n",
    "    # Plot threshold vs metric\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, scores, marker='o', markersize=3, linewidth=2)\n",
    "    plt.axvline(x=best_threshold, color='r', linestyle='--', label=f'Best threshold: {best_threshold:.2f}')\n",
    "    plt.axhline(y=best_score, color='g', linestyle='--', label=f'Best {metric}: {best_score:.4f}')\n",
    "    plt.title(f'{metric.capitalize()} vs. Threshold', fontsize=14)\n",
    "    plt.xlabel('Threshold', fontsize=12)\n",
    "    plt.ylabel(metric.capitalize(), fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.savefig(f'figures/model_evaluation/threshold_tuning_{metric}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Optimal threshold for {metric}: {best_threshold:.4f}, {metric}: {best_score:.4f}\")\n",
    "    return best_threshold\n",
    "\n",
    "# Enhanced plotting functions\n",
    "def plot_roc_curve_enhanced(models, X_val, y_val, figsize=(12, 8)):\n",
    "    \"\"\"\n",
    "    Plot enhanced ROC curves for multiple models with AUC and confidence intervals.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Random classifier line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random')\n",
    "    \n",
    "    # Plot for each model\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "            fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            \n",
    "            # Plot ROC curve\n",
    "            plt.plot(fpr, tpr, lw=2, \n",
    "                     label=f'{name} (AUC = {roc_auc:.4f})')\n",
    "        except Exception as e:\n",
    "            print(f\"Could not plot ROC curve for {name}: {e}\")\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curves', fontsize=15)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('figures/model_evaluation/enhanced_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall_curve_enhanced(models, X_val, y_val, figsize=(12, 8)):\n",
    "    \"\"\"\n",
    "    Plot enhanced precision-recall curves for multiple models.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Calculate baseline (no skill classifier)\n",
    "    no_skill = len(y_val[y_val == 1]) / len(y_val)\n",
    "    plt.plot([0, 1], [no_skill, no_skill], 'k--', lw=2, label=f'No Skill ({no_skill:.3f})')\n",
    "    \n",
    "    # Plot for each model\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "            precision, recall, _ = precision_recall_curve(y_val, y_pred_proba)\n",
    "            pr_auc = auc(recall, precision)\n",
    "            avg_precision = average_precision_score(y_val, y_pred_proba)\n",
    "            \n",
    "            # Plot PR curve\n",
    "            plt.plot(recall, precision, lw=2, \n",
    "                     label=f'{name} (AP = {avg_precision:.4f}, AUC = {pr_auc:.4f})')\n",
    "        except Exception as e:\n",
    "            print(f\"Could not plot PR curve for {name}: {e}\")\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall', fontsize=12)\n",
    "    plt.ylabel('Precision', fontsize=12)\n",
    "    plt.title('Precision-Recall Curves', fontsize=15)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('figures/model_evaluation/enhanced_pr_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix_enhanced(cm, model_name, figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Plot an enhanced confusion matrix with additional metrics.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Extract values from confusion matrix\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['No Stroke', 'Stroke'],\n",
    "                yticklabels=['No Stroke', 'Stroke'])\n",
    "    \n",
    "    # Add metrics as text\n",
    "    plt.text(0.5, -0.15, f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f}\", \n",
    "             ha='center', fontsize=12, transform=plt.gca().transAxes)\n",
    "    plt.text(0.5, -0.2, f\"Recall: {recall:.4f} | Specificity: {specificity:.4f} | F1: {f1:.4f}\", \n",
    "             ha='center', fontsize=12, transform=plt.gca().transAxes)\n",
    "    \n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.title(f'Confusion Matrix - {model_name}', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figures/model_evaluation/enhanced_cm_{model_name.replace(\" \", \"_\").lower()}.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importance_enhanced(model, feature_names, model_name, figsize=(12, 10), top_n=20):\n",
    "    \"\"\"\n",
    "    Enhanced feature importance plot with sorted bars and annotations.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get feature importances\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "        elif hasattr(model, 'coef_'):\n",
    "            importances = np.abs(model.coef_[0])\n",
    "        else:\n",
    "            print(f\"{model_name} does not have feature importances.\")\n",
    "            return None\n",
    "        \n",
    "        # Create DataFrame of feature importances\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "        })\n",
    "        \n",
    "        # Sort by importance and get top N\n",
    "        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "        top_features = feature_importance.head(top_n)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=figsize)\n",
    "        ax = sns.barplot(x='Importance', y='Feature', data=top_features, palette='viridis')\n",
    "        \n",
    "        # Add values as annotations\n",
    "        for i, v in enumerate(top_features['Importance']):\n",
    "            ax.text(v + 0.001, i, f\"{v:.4f}\", va='center', fontsize=10)\n",
    "        \n",
    "        plt.title(f'Top {top_n} Feature Importance - {model_name}', fontsize=15)\n",
    "        plt.xlabel('Importance', fontsize=12)\n",
    "        plt.ylabel('Feature', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'figures/model_evaluation/enhanced_importance_{model_name.replace(\" \", \"_\").lower()}.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return feature_importance\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting feature importance for {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to apply and evaluate different resampling methods\n",
    "def apply_resampling_methods(X_train_scaled, y_train):\n",
    "    resampled_data = {}\n",
    "    \n",
    "    # 1. SMOTE with different settings\n",
    "    print(\"Applying SMOTE...\")\n",
    "    smote = SMOTE(sampling_strategy=0.5, random_state=SEED, k_neighbors=5)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "    resampled_data['smote'] = (X_train_smote, y_train_smote)\n",
    "    \n",
    "    # 2. BorderlineSMOTE\n",
    "    print(\"Applying BorderlineSMOTE...\")\n",
    "    bsmote = BorderlineSMOTE(sampling_strategy=0.5, random_state=SEED, k_neighbors=5)\n",
    "    X_train_bsmote, y_train_bsmote = bsmote.fit_resample(X_train_scaled, y_train)\n",
    "    resampled_data['bsmote'] = (X_train_bsmote, y_train_bsmote)\n",
    "\n",
    "    # 3. ADASYN for more advanced synthetic data generation\n",
    "    print(\"Applying ADASYN...\")\n",
    "    adasyn = ADASYN(sampling_strategy=0.5, random_state=SEED, n_neighbors=5)\n",
    "    X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_scaled, y_train)\n",
    "    resampled_data['adasyn'] = (X_train_adasyn, y_train_adasyn)\n",
    "\n",
    "    # 4. SMOTETomek (combines oversampling with cleaning)\n",
    "    print(\"Applying SMOTETomek...\")\n",
    "    smotetomek = SMOTETomek(sampling_strategy=0.5, random_state=SEED)\n",
    "    X_train_smotetomek, y_train_smotetomek = smotetomek.fit_resample(X_train_scaled, y_train)\n",
    "    resampled_data['smotetomek'] = (X_train_smotetomek, y_train_smotetomek)\n",
    "\n",
    "    # 5. SMOTEENN (combines SMOTE with Edited Nearest Neighbors)\n",
    "    print(\"Applying SMOTEENN...\")\n",
    "    smoteenn = SMOTEENN(sampling_strategy=0.5, random_state=SEED)\n",
    "    X_train_smoteenn, y_train_smoteenn = smoteenn.fit_resample(X_train_scaled, y_train)\n",
    "    resampled_data['smoteenn'] = (X_train_smoteenn, y_train_smoteenn)\n",
    "\n",
    "    # 6. More aggressive SMOTE (1:1 ratio)\n",
    "    print(\"Applying aggressive SMOTE (1:1 ratio)...\")\n",
    "    smote_11 = SMOTE(sampling_strategy=1.0, random_state=SEED, k_neighbors=3)\n",
    "    X_train_smote_11, y_train_smote_11 = smote_11.fit_resample(X_train_scaled, y_train)\n",
    "    resampled_data['smote_11'] = (X_train_smote_11, y_train_smote_11)\n",
    "    \n",
    "    # Print sizes and class distributions\n",
    "    for name, (X_resampled, y_resampled) in resampled_data.items():\n",
    "        pos_count = np.sum(y_resampled == 1)\n",
    "        total = len(y_resampled)\n",
    "        print(f\"{name}: {X_resampled.shape}, {pos_count}/{total} positive cases ({pos_count/total*100:.2f}%)\")\n",
    "    \n",
    "    return resampled_data\n",
    "\n",
    "# Function to train models on different resampled data\n",
    "def train_models_on_resampled_data(models, resampled_data, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Train models on different resampled datasets and evaluate on validation set.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for resampling_method, (X_resampled, y_resampled) in resampled_data.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training models on {resampling_method} resampled data\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        method_results = {'models': {}, 'metrics': {}}\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            print(f\"\\nTraining {model_name} on {resampling_method} data...\")\n",
    "            \n",
    "            # Clone the model to avoid contamination\n",
    "            model_clone = clone(model)\n",
    "            \n",
    "            # Train the model\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                model_clone.fit(X_resampled, y_resampled)\n",
    "                training_time = time.time() - start_time\n",
    "                \n",
    "                # Evaluate on validation set\n",
    "                metrics = evaluate_model_comprehensive(\n",
    "                    model_clone, X_val, y_val, \n",
    "                    f\"{model_name} ({resampling_method})\"\n",
    "                )\n",
    "                metrics['training_time'] = training_time\n",
    "                \n",
    "                # Store model and metrics\n",
    "                method_results['models'][model_name] = model_clone\n",
    "                method_results['metrics'][model_name] = metrics\n",
    "                \n",
    "                print(f\"Training time: {training_time:.2f} seconds\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error training {model_name} on {resampling_method} data: {e}\")\n",
    "        \n",
    "        results[resampling_method] = method_results\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create optimized models\n",
    "def create_optimized_models():\n",
    "    \"\"\"Create models with optimized configurations for imbalanced data.\"\"\"\n",
    "    optimized_models = {}\n",
    "    \n",
    "    # 1. Logistic Regression with class weights and regularization\n",
    "    optimized_models['LR_Optimized'] = LogisticRegression(\n",
    "        C=0.1,  # Stronger regularization\n",
    "        penalty='l1',  # L1 regularization for feature selection\n",
    "        solver='liblinear',\n",
    "        class_weight={0: 1, 1: 10},  # Higher weight for minority class\n",
    "        max_iter=5000,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    \n",
    "    # 2. Random Forest with balanced class weights\n",
    "    optimized_models['RF_Optimized'] = RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True,\n",
    "        oob_score=True,\n",
    "        class_weight='balanced_subsample',  # Balanced weights at each bootstrap iteration\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # 3. XGBoost with imbalanced settings\n",
    "    optimized_models['XGB_Optimized'] = xgb.XGBClassifier(\n",
    "        learning_rate=0.01,\n",
    "        n_estimators=1000,\n",
    "        max_depth=4,\n",
    "        min_child_weight=2,\n",
    "        gamma=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=20,  # Weight for positive class\n",
    "        objective='binary:logistic',\n",
    "        tree_method='exact',  # Use 'gpu_hist' if GPU available\n",
    "        random_state=SEED,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    # 4. LightGBM with imbalanced settings\n",
    "    optimized_models['LGBM_Optimized'] = lgb.LGBMClassifier(\n",
    "        boosting_type='goss',  # Gradient-based One-Side Sampling\n",
    "        learning_rate=0.01,\n",
    "        n_estimators=1000,\n",
    "        num_leaves=31,\n",
    "        max_depth=5,\n",
    "        min_child_samples=20,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        is_unbalance=True,  # Handle imbalanced dataset\n",
    "        random_state=SEED,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    # 5. CatBoost with imbalanced settings\n",
    "    optimized_models['CatBoost_Optimized'] = cb.CatBoostClassifier(\n",
    "        iterations=500,\n",
    "        learning_rate=0.02,\n",
    "        depth=6,\n",
    "        l2_leaf_reg=3,\n",
    "        bootstrap_type='Bernoulli',\n",
    "        subsample=0.8,\n",
    "        scale_pos_weight=15,  # Weight for positive class\n",
    "        random_state=SEED,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # 6. Balanced Random Forest (from imbalanced-learn)\n",
    "    optimized_models['Balanced_RF'] = BalancedRandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=8,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True,\n",
    "        sampling_strategy='auto',\n",
    "        replacement=False,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # 7. RUSBoost (boosting with undersampling)\n",
    "    optimized_models['RUSBoost'] = RUSBoostClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.1,\n",
    "        algorithm='SAMME.R',\n",
    "        sampling_strategy='auto',\n",
    "        replacement=False,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    \n",
    "    # 8. EasyEnsemble - FIXED version without base_estimator parameter\n",
    "    optimized_models['EasyEnsemble'] = EasyEnsembleClassifier(\n",
    "        n_estimators=10,\n",
    "        sampling_strategy='auto',\n",
    "        replacement=False,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # 9. Gradient Boosting with calibration\n",
    "    gb = GradientBoostingClassifier(\n",
    "        learning_rate=0.01,\n",
    "        n_estimators=1000,\n",
    "        max_depth=3,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        subsample=0.8,\n",
    "        max_features='sqrt',\n",
    "        random_state=SEED\n",
    "    )\n",
    "    # FIXED: Changed base_estimator to estimator\n",
    "    optimized_models['GB_Calibrated'] = CalibratedClassifierCV(\n",
    "        estimator=gb,  # Changed from base_estimator to estimator\n",
    "        method='sigmoid',\n",
    "        cv=5\n",
    "    )\n",
    "    \n",
    "    # 10. SVM with class weights and probability calibration\n",
    "    svc = SVC(\n",
    "        C=1.0,\n",
    "        kernel='rbf',\n",
    "        gamma='scale',\n",
    "        class_weight='balanced',\n",
    "        probability=True,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    # FIXED: Changed base_estimator to estimator\n",
    "    optimized_models['SVM_Calibrated'] = CalibratedClassifierCV(\n",
    "        estimator=svc,  # Changed from base_estimator to estimator\n",
    "        method='sigmoid',\n",
    "        cv=5\n",
    "    )\n",
    "    \n",
    "    return optimized_models\n",
    "\n",
    "# Create stack/ensemble models\n",
    "def create_ensemble_models(base_models):\n",
    "    \"\"\"Create advanced ensemble models from base models.\"\"\"\n",
    "    ensemble_models = {}\n",
    "    \n",
    "    # 1. Simple Voting Ensemble (hard voting)\n",
    "    ensemble_models['Voting_Hard'] = VotingClassifier(\n",
    "        estimators=list(base_models.items()),\n",
    "        voting='hard',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # 2. Soft Voting Ensemble with weights\n",
    "    # Use inverse of log loss or F1 as weights\n",
    "    # For simplicity, using equal weights here\n",
    "    ensemble_models['Voting_Soft'] = VotingClassifier(\n",
    "        estimators=list(base_models.items()),\n",
    "        voting='soft',\n",
    "        weights=[1] * len(base_models),\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # 3. Stacking Ensemble with LogisticRegression meta-learner\n",
    "    ensemble_models['Stacking_LR'] = StackingClassifier(\n",
    "        estimators=list(base_models.items()),\n",
    "        final_estimator=LogisticRegression(class_weight='balanced'),\n",
    "        cv=5,\n",
    "        stack_method='predict_proba',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # 4. Stacking Ensemble with Random Forest meta-learner\n",
    "    ensemble_models['Stacking_RF'] = StackingClassifier(\n",
    "        estimators=list(base_models.items()),\n",
    "        final_estimator=RandomForestClassifier(\n",
    "            n_estimators=100, \n",
    "            class_weight='balanced',\n",
    "            random_state=SEED\n",
    "        ),\n",
    "        cv=5,\n",
    "        stack_method='predict_proba',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # 5. Multi-level stacking (creating a more complex ensemble)\n",
    "    # First level stacking\n",
    "    level1_stack = StackingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', base_models.get('LR_Optimized', LogisticRegression(class_weight='balanced'))),\n",
    "            ('rf', base_models.get('RF_Optimized', RandomForestClassifier(class_weight='balanced'))),\n",
    "            ('xgb', base_models.get('XGB_Optimized', xgb.XGBClassifier())),\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(),\n",
    "        cv=5,\n",
    "        stack_method='predict_proba'\n",
    "    )\n",
    "    \n",
    "    # Second level stacking\n",
    "    ensemble_models['Deep_Stacking'] = StackingClassifier(\n",
    "        estimators=[\n",
    "            ('level1', level1_stack),\n",
    "            ('lgbm', base_models.get('LGBM_Optimized', lgb.LGBMClassifier())),\n",
    "            ('cb', base_models.get('CatBoost_Optimized', cb.CatBoostClassifier(verbose=0))),\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(class_weight='balanced'),\n",
    "        cv=5,\n",
    "        stack_method='predict_proba'\n",
    "    )\n",
    "    \n",
    "    # 6. Bagging with sampling adjustment\n",
    "    ensemble_models['Bagging_Adjusted'] = BaggingClassifier(\n",
    "        base_estimator=LogisticRegression(class_weight='balanced'),\n",
    "        n_estimators=100,\n",
    "        max_samples=0.8,  # Use 80% of samples\n",
    "        max_features=0.8,  # Use 80% of features\n",
    "        bootstrap=True,\n",
    "        bootstrap_features=True,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    return ensemble_models\n",
    "\n",
    "# Function to extract best models based on a specific metric\n",
    "def extract_best_models(training_results, metric='f1', top_n=3):\n",
    "    \"\"\"\n",
    "    Extract the top N models based on a specific metric.\n",
    "    \"\"\"\n",
    "    all_models = []\n",
    "    \n",
    "    for resampling_method, result in training_results.items():\n",
    "        for model_name, metrics in result['metrics'].items():\n",
    "            all_models.append({\n",
    "                'resampling_method': resampling_method,\n",
    "                'model_name': model_name,\n",
    "                'model': result['models'][model_name],\n",
    "                metric: metrics[metric],\n",
    "                'metrics': metrics\n",
    "            })\n",
    "    \n",
    "    # Sort by the specified metric\n",
    "    all_models.sort(key=lambda x: x[metric], reverse=True)\n",
    "    \n",
    "    # Return top N models\n",
    "    return all_models[:top_n]\n",
    "\n",
    "# Create a prediction function for new data\n",
    "def predict_stroke_risk(input_data, model_path='models/final_stroke_prediction_model.pkl', \n",
    "                        scaler_path='models/final_stroke_prediction_scaler.pkl',\n",
    "                        features_path='models/selected_features.pkl',\n",
    "                        threshold_path='models/optimal_threshold.pkl'):\n",
    "    \"\"\"\n",
    "    Make stroke predictions on new data using the trained model.\n",
    "    \"\"\"\n",
    "    # Load the model, scaler, and optimal threshold\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    with open(scaler_path, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    \n",
    "    with open(threshold_path, 'rb') as f:\n",
    "        threshold = pickle.load(f)\n",
    "    \n",
    "    # Check if feature selection was used\n",
    "    try:\n",
    "        with open(features_path, 'rb') as f:\n",
    "            selected_features = pickle.load(f)\n",
    "        \n",
    "        # Select the features from input data\n",
    "        input_data = input_data[selected_features]\n",
    "    except:\n",
    "        print(\"No feature selection was used or file not found.\")\n",
    "    \n",
    "    # Scale the input data\n",
    "    input_data_scaled = scaler.transform(input_data)\n",
    "    \n",
    "    # Get prediction probabilities\n",
    "    probabilities = model.predict_proba(input_data_scaled)[:, 1]\n",
    "    \n",
    "    # Apply the optimal threshold\n",
    "    predictions = (probabilities >= threshold).astype(int)\n",
    "    \n",
    "    return predictions, probabilities\n",
    "\n",
    "def main():\n",
    "    # Load the preprocessed and encoded dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv('data/processed/stroke_dataset_encoded.csv')\n",
    "\n",
    "    # Display basic information\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Number of features: {df.shape[1] - 1}\")\n",
    "\n",
    "    # Target variable distribution\n",
    "    print(\"\\nTarget distribution:\")\n",
    "    print(df['stroke'].value_counts())\n",
    "    print(df['stroke'].value_counts(normalize=True) * 100)\n",
    "\n",
    "    # Check for any missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(\"\\nMissing values:\")\n",
    "        print(missing_values[missing_values > 0])\n",
    "    else:\n",
    "        print(\"\\nNo missing values found except for comorbidity.\")\n",
    "\n",
    "    # Check the comorbidity column if it exists\n",
    "    if 'comorbidity' in df.columns:\n",
    "        print(f\"\\nComorbidity column unique values: {df['comorbidity'].unique()}\")\n",
    "        print(f\"Comorbidity column missing values: {df['comorbidity'].isna().sum()}\")\n",
    "\n",
    "    # Get a list of all numerical features\n",
    "    numerical_features = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    if 'stroke' in numerical_features:\n",
    "        numerical_features.remove('stroke')\n",
    "    if 'comorbidity' in numerical_features:\n",
    "        numerical_features.remove('comorbidity')\n",
    "\n",
    "    print(f\"\\nNumerical features: {numerical_features}\")\n",
    "\n",
    "    # Get a list of all categorical features\n",
    "    categorical_features = df.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "    print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "    # Additional binary features (encoded as 0/1 but actually categorical)\n",
    "    binary_features = [col for col in numerical_features if df[col].nunique() == 2]\n",
    "    print(f\"Binary features: {binary_features}\")\n",
    "\n",
    "    # Perform advanced feature engineering\n",
    "    print(\"\\nPerforming advanced feature engineering...\")\n",
    "\n",
    "    # Create a copy of the dataframe\n",
    "    df_engineered = df.copy()\n",
    "\n",
    "    # Drop problematic columns if they exist\n",
    "    if 'comorbidity' in df_engineered.columns:\n",
    "        df_engineered = df_engineered.drop('comorbidity', axis=1)\n",
    "    if 'id' in df_engineered.columns:\n",
    "        df_engineered = df_engineered.drop('id', axis=1)\n",
    "\n",
    "    # Create advanced features\n",
    "\n",
    "    # 1. Age-related transformations\n",
    "    df_engineered['age_squared'] = df_engineered['age'] ** 2\n",
    "    df_engineered['age_cube'] = df_engineered['age'] ** 3\n",
    "    df_engineered['log_age'] = np.log1p(df_engineered['age'])\n",
    "\n",
    "    # 2. BMI-related features\n",
    "    if 'bmi' in df_engineered.columns:\n",
    "        df_engineered['bmi_squared'] = df_engineered['bmi'] ** 2\n",
    "        df_engineered['age_bmi_ratio'] = df_engineered['age'] / df_engineered['bmi']\n",
    "        df_engineered['bmi_age_product'] = df_engineered['age'] * df_engineered['bmi'] / 100\n",
    "        \n",
    "        # BMI risk categories (more detailed)\n",
    "        df_engineered['bmi_risk'] = pd.cut(\n",
    "            df_engineered['bmi'], \n",
    "            bins=[0, 18.5, 25, 30, 35, 100], \n",
    "            labels=[1, 0, 2, 3, 4]\n",
    "        ).astype(float)\n",
    "        \n",
    "        # Age-adjusted BMI\n",
    "        df_engineered['age_adj_bmi'] = df_engineered['bmi'] * (1 + (df_engineered['age'] / 100))\n",
    "\n",
    "    # 3. Glucose-related features\n",
    "    if 'avg_glucose_level' in df_engineered.columns:\n",
    "        df_engineered['glucose_squared'] = df_engineered['avg_glucose_level'] ** 2\n",
    "        df_engineered['log_glucose'] = np.log1p(df_engineered['avg_glucose_level'])\n",
    "        df_engineered['glucose_bmi_product'] = df_engineered['avg_glucose_level'] * df_engineered['bmi'] / 100\n",
    "        df_engineered['glucose_age_ratio'] = df_engineered['avg_glucose_level'] / df_engineered['age']\n",
    "        \n",
    "        # Glucose risk categories\n",
    "        df_engineered['glucose_risk'] = pd.cut(\n",
    "            df_engineered['avg_glucose_level'],\n",
    "            bins=[0, 70, 100, 126, 200, 500],\n",
    "            labels=[2, 0, 1, 3, 4]\n",
    "        ).astype(float)\n",
    "\n",
    "    # 4. Hypertension and heart disease interactions\n",
    "    if 'hypertension' in df_engineered.columns and 'heart_disease' in df_engineered.columns:\n",
    "        df_engineered['health_risk_combined'] = df_engineered['hypertension'] + df_engineered['heart_disease']\n",
    "        df_engineered['hypertension_heart_disease'] = df_engineered['hypertension'] * df_engineered['heart_disease']\n",
    "        \n",
    "        # Age-adjusted health risks\n",
    "        df_engineered['age_hypertension_risk'] = df_engineered['age'] * df_engineered['hypertension'] / 10\n",
    "        df_engineered['age_heart_disease_risk'] = df_engineered['age'] * df_engineered['heart_disease'] / 10\n",
    "        \n",
    "        # Combined age and health risks\n",
    "        df_engineered['combined_health_age_risk'] = df_engineered['age'] * (1 + df_engineered['hypertension'] + df_engineered['heart_disease'])\n",
    "\n",
    "    # 5. Complex feature combinations\n",
    "    # Risk score based on all factors\n",
    "    df_engineered['comprehensive_risk_score'] = (\n",
    "        df_engineered['age'] / 10 +\n",
    "        df_engineered['bmi'] / 5 +\n",
    "        df_engineered['avg_glucose_level'] / 20\n",
    "    )\n",
    "\n",
    "    if 'hypertension' in df_engineered.columns and 'heart_disease' in df_engineered.columns:\n",
    "        df_engineered['comprehensive_risk_score'] += (\n",
    "            df_engineered['hypertension'] * 5 +\n",
    "            df_engineered['heart_disease'] * 5\n",
    "        )\n",
    "\n",
    "    if 'ever_married_Yes' in df_engineered.columns:\n",
    "        df_engineered['comprehensive_risk_score'] += df_engineered['ever_married_Yes'] * 1\n",
    "\n",
    "    # 6. Create interaction features between numerical and binary features\n",
    "    for num_feat in ['age', 'bmi', 'avg_glucose_level']:\n",
    "        if num_feat in df_engineered.columns:\n",
    "            for bin_feat in binary_features:\n",
    "                if bin_feat in df_engineered.columns and bin_feat != num_feat:\n",
    "                    df_engineered[f'{num_feat}_{bin_feat}_interaction'] = df_engineered[num_feat] * df_engineered[bin_feat]\n",
    "\n",
    "    # 7. Create polynomial features for key numerical variables\n",
    "    # Select key numerical features for polynomial expansion\n",
    "    poly_features = ['age', 'bmi', 'avg_glucose_level']\n",
    "    poly_features = [f for f in poly_features if f in df_engineered.columns]\n",
    "\n",
    "    if poly_features:\n",
    "        # Create polynomial features (degree 2)\n",
    "        poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "        poly_features_array = poly.fit_transform(df_engineered[poly_features])\n",
    "        \n",
    "        # Get the feature names\n",
    "        poly_feature_names = poly.get_feature_names_out(poly_features)\n",
    "        \n",
    "        # Add polynomial features to the dataframe\n",
    "        # Skip the first few that are just the original features\n",
    "        for i, name in enumerate(poly_feature_names[len(poly_features):], len(poly_features)):\n",
    "            df_engineered[f'poly_{name}'] = poly_features_array[:, i]\n",
    "\n",
    "    # Display the engineered dataframe info\n",
    "    print(f\"\\nOriginal dataframe shape: {df.shape}\")\n",
    "    print(f\"Engineered dataframe shape: {df_engineered.shape}\")\n",
    "    print(f\"Number of new features added: {df_engineered.shape[1] - df.shape[1]}\")\n",
    "\n",
    "    # List the new features\n",
    "    new_features = set(df_engineered.columns) - set(df.columns)\n",
    "    print(f\"\\nNew features added: {sorted(new_features)}\")\n",
    "\n",
    "    # Separate features and target\n",
    "    X = df_engineered.drop('stroke', axis=1)\n",
    "    y = df_engineered['stroke']\n",
    "\n",
    "    # Split the data into training, validation, and test sets\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    "    )\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full, test_size=0.25, random_state=SEED, stratify=y_train_full\n",
    "    )\n",
    "\n",
    "    print(f\"Training set: {X_train.shape}, {y_train.value_counts()[1]}/{len(y_train)} positive cases ({y_train.mean()*100:.2f}%)\")\n",
    "    print(f\"Validation set: {X_val.shape}, {y_val.value_counts()[1]}/{len(y_val)} positive cases ({y_val.mean()*100:.2f}%)\")\n",
    "    print(f\"Test set: {X_test.shape}, {y_test.value_counts()[1]}/{len(y_test)} positive cases ({y_test.mean()*100:.2f}%)\")\n",
    "\n",
    "    # Feature scaling - try different scalers\n",
    "    scalers = {\n",
    "        'standard': StandardScaler(),\n",
    "        'minmax': MinMaxScaler(),\n",
    "        'robust': RobustScaler()\n",
    "    }\n",
    "\n",
    "    scaled_data = {}\n",
    "    for scaler_name, scaler in scalers.items():\n",
    "        # Fit on training data\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        scaled_data[scaler_name] = {\n",
    "            'train': X_train_scaled,\n",
    "            'val': X_val_scaled,\n",
    "            'test': X_test_scaled,\n",
    "            'scaler': scaler\n",
    "        }\n",
    "\n",
    "    print(\"Data scaled with multiple methods: Standard, MinMax, and Robust scaling\")\n",
    "\n",
    "    # Convert back to DataFrames with column names for easier handling\n",
    "    for scaler_name in scaled_data:\n",
    "        scaled_data[scaler_name]['train_df'] = pd.DataFrame(\n",
    "            scaled_data[scaler_name]['train'], \n",
    "            columns=X_train.columns\n",
    "        )\n",
    "        scaled_data[scaler_name]['val_df'] = pd.DataFrame(\n",
    "            scaled_data[scaler_name]['val'], \n",
    "            columns=X_val.columns\n",
    "        )\n",
    "        scaled_data[scaler_name]['test_df'] = pd.DataFrame(\n",
    "            scaled_data[scaler_name]['test'], \n",
    "            columns=X_test.columns\n",
    "        )\n",
    "\n",
    "    # Apply resampling methods\n",
    "    print(\"Applying advanced resampling techniques...\")\n",
    "    resampled_data = apply_resampling_methods(scaled_data['standard']['train'], y_train)\n",
    "\n",
    "    # Visualize class distributions after resampling\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.subplot(2, 4, 1)\n",
    "    plt.title(\"Original\")\n",
    "    plt.pie([len(y_train) - sum(y_train), sum(y_train)], labels=[\"0\", \"1\"], autopct='%1.1f%%', colors=['#82CAFA', '#FE6B64'])\n",
    "\n",
    "    positions = [(2, 4, 2), (2, 4, 3), (2, 4, 4), (2, 4, 5), (2, 4, 6), (2, 4, 7)]\n",
    "    for (i, j, k), (method, (_, y_resampled)) in zip(positions, list(resampled_data.items())[:6]):\n",
    "        plt.subplot(i, j, k)\n",
    "        plt.title(method)\n",
    "        plt.pie([len(y_resampled) - sum(y_resampled), sum(y_resampled)], \n",
    "                labels=[\"0\", \"1\"], autopct='%1.1f%%', colors=['#82CAFA', '#FE6B64'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/model_evaluation/resampling_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Create optimized models\n",
    "    print(\"Creating optimized models...\")\n",
    "    optimized_models = create_optimized_models()\n",
    "\n",
    "    # Train models on resampled data\n",
    "    print(\"Training models on different resampled datasets...\")\n",
    "    # Use a subset of models to speed up the process\n",
    "    models_to_train = {\n",
    "        'LR_Optimized': optimized_models['LR_Optimized'],\n",
    "        'RF_Optimized': optimized_models['RF_Optimized'],\n",
    "        'XGB_Optimized': optimized_models['XGB_Optimized'],\n",
    "        'LGBM_Optimized': optimized_models['LGBM_Optimized'],\n",
    "        'Balanced_RF': optimized_models['Balanced_RF']\n",
    "    }\n",
    "\n",
    "    # Use a subset of resampling methods\n",
    "    resampling_methods_to_use = {\n",
    "        'smote': resampled_data['smote'],\n",
    "        'smotetomek': resampled_data['smotetomek'],\n",
    "        'smoteenn': resampled_data['smoteenn']\n",
    "    }\n",
    "\n",
    "    # Train models\n",
    "    training_results = train_models_on_resampled_data(\n",
    "        models_to_train, \n",
    "        resampling_methods_to_use,\n",
    "        scaled_data['standard']['val'], \n",
    "        y_val\n",
    "    )\n",
    "\n",
    "    # Extract best models based on F1 score\n",
    "    best_models_f1 = extract_best_models(training_results, metric='f1', top_n=5)\n",
    "\n",
    "    print(\"\\nTop 5 models based on F1 score:\")\n",
    "    for i, model_info in enumerate(best_models_f1, 1):\n",
    "        print(f\"{i}. {model_info['model_name']} with {model_info['resampling_method']}: F1 = {model_info['f1']:.4f}\")\n",
    "\n",
    "    # Extract best models based on balanced accuracy\n",
    "    best_models_bal_acc = extract_best_models(training_results, metric='balanced_accuracy', top_n=5)\n",
    "\n",
    "    print(\"\\nTop 5 models based on balanced accuracy:\")\n",
    "    for i, model_info in enumerate(best_models_bal_acc, 1):\n",
    "        print(f\"{i}. {model_info['model_name']} with {model_info['resampling_method']}: Balanced Accuracy = {model_info['balanced_accuracy']:.4f}\")\n",
    "\n",
    "    # Extract best models based on ROC AUC\n",
    "    best_models_roc_auc = extract_best_models(training_results, metric='roc_auc', top_n=5)\n",
    "\n",
    "    print(\"\\nTop 5 models based on ROC AUC:\")\n",
    "    for i, model_info in enumerate(best_models_roc_auc, 1):\n",
    "        print(f\"{i}. {model_info['model_name']} with {model_info['resampling_method']}: ROC AUC = {model_info['roc_auc']:.4f}\")\n",
    "\n",
    "    # Compile top performing models for ensemble\n",
    "    top_performing_models = {}\n",
    "    for model_info in best_models_f1[:3]:  # Take top 3 from F1\n",
    "        model_key = f\"{model_info['model_name']}_{model_info['resampling_method']}\"\n",
    "        top_performing_models[model_key] = model_info['model']\n",
    "\n",
    "    for model_info in best_models_roc_auc[:2]:  # Take top 2 from ROC AUC\n",
    "        if model_info not in best_models_f1[:3]:  # Avoid duplicates\n",
    "            model_key = f\"{model_info['model_name']}_{model_info['resampling_method']}\"\n",
    "            top_performing_models[model_key] = model_info['model']\n",
    "\n",
    "    print(f\"\\nCollected {len(top_performing_models)} unique top-performing models for ensemble creation.\")\n",
    "\n",
    "    # Create ensembles from top performing models\n",
    "    print(\"Creating ensemble models from top performers...\")\n",
    "    ensemble_models = create_ensemble_models(top_performing_models)\n",
    "\n",
    "    # Train and evaluate ensemble models\n",
    "    ensemble_results = {}\n",
    "    for ensemble_name, ensemble_model in ensemble_models.items():\n",
    "        print(f\"\\nTraining {ensemble_name}...\")\n",
    "        \n",
    "        # Train the ensemble\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            # Use the original training data for ensemble model training\n",
    "            ensemble_model.fit(scaled_data['standard']['train'], y_train)\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            metrics = evaluate_model_comprehensive(\n",
    "                ensemble_model, scaled_data['standard']['val'], y_val, \n",
    "                ensemble_name\n",
    "            )\n",
    "            metrics['training_time'] = training_time\n",
    "            \n",
    "            # Store ensemble and metrics\n",
    "            ensemble_results[ensemble_name] = {\n",
    "                'model': ensemble_model,\n",
    "                'metrics': metrics\n",
    "            }\n",
    "            \n",
    "            print(f\"Training time: {training_time:.2f} seconds\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error training {ensemble_name}: {e}\")\n",
    "\n",
    "    # Find the best ensemble model\n",
    "    best_ensemble = None\n",
    "    best_ensemble_f1 = 0\n",
    "\n",
    "    for ensemble_name, result in ensemble_results.items():\n",
    "        f1 = result['metrics']['f1']\n",
    "        if f1 > best_ensemble_f1:\n",
    "            best_ensemble_f1 = f1\n",
    "            best_ensemble = (ensemble_name, result['model'])\n",
    "\n",
    "    print(f\"\\nBest ensemble: {best_ensemble[0]} with F1 = {best_ensemble_f1:.4f}\")\n",
    "\n",
    "    # Compare top individual models with ensemble models\n",
    "    compare_models = {}\n",
    "\n",
    "    # Add top individual models\n",
    "    for model_info in best_models_f1[:3]:\n",
    "        model_key = f\"{model_info['model_name']} ({model_info['resampling_method']})\"\n",
    "        compare_models[model_key] = model_info['model']\n",
    "\n",
    "    # Add top ensemble models\n",
    "    for ensemble_name, result in ensemble_results.items():\n",
    "        if result['metrics']['f1'] >= 0.8 * best_ensemble_f1:  # Include ensembles with at least 80% of the best F1\n",
    "            compare_models[ensemble_name] = result['model']\n",
    "\n",
    "    # Plot ROC and PR curves for comparison\n",
    "    plot_roc_curve_enhanced(compare_models, scaled_data['standard']['val'], y_val)\n",
    "    plot_precision_recall_curve_enhanced(compare_models, scaled_data['standard']['val'], y_val)\n",
    "\n",
    "    # Create a performance comparison dataframe\n",
    "    performance_df = pd.DataFrame(columns=['Model', 'Type', 'Accuracy', 'Balanced_Accuracy', 'Precision', \n",
    "                                         'Recall', 'F1', 'ROC_AUC', 'G_mean'])\n",
    "\n",
    "    # Add individual models\n",
    "    for model_info in best_models_f1:\n",
    "        model_key = f\"{model_info['model_name']} ({model_info['resampling_method']})\"\n",
    "        metrics = model_info['metrics']\n",
    "        performance_df = pd.concat([performance_df, pd.DataFrame([{\n",
    "            'Model': model_key,\n",
    "            'Type': 'Individual',\n",
    "            'Accuracy': metrics['accuracy'],\n",
    "            'Balanced_Accuracy': metrics['balanced_accuracy'],\n",
    "            'Precision': metrics['precision'],\n",
    "            'Recall': metrics['recall'],\n",
    "            'F1': metrics['f1'],\n",
    "            'ROC_AUC': metrics.get('roc_auc', np.nan),\n",
    "            'G_mean': metrics['g_mean']\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    # Add ensemble models\n",
    "    for ensemble_name, result in ensemble_results.items():\n",
    "        metrics = result['metrics']\n",
    "        performance_df = pd.concat([performance_df, pd.DataFrame([{\n",
    "            'Model': ensemble_name,\n",
    "            'Type': 'Ensemble',\n",
    "            'Accuracy': metrics['accuracy'],\n",
    "            'Balanced_Accuracy': metrics['balanced_accuracy'],\n",
    "            'Precision': metrics['precision'],\n",
    "            'Recall': metrics['recall'],\n",
    "            'F1': metrics['f1'],\n",
    "            'ROC_AUC': metrics.get('roc_auc', np.nan),\n",
    "            'G_mean': metrics['g_mean']\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    # Sort by F1 score\n",
    "    performance_df = performance_df.sort_values('F1', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Save the performance comparison\n",
    "    performance_df.to_csv('results/model_comparison.csv', index=False)\n",
    "\n",
    "    # Display the performance comparison\n",
    "    print(\"\\nModel Performance Comparison:\")\n",
    "    print(performance_df)\n",
    "\n",
    "    # Visualize the performance comparison\n",
    "    plt.figure(figsize=(14, 10))\n",
    "\n",
    "    metrics_to_plot = ['F1', 'ROC_AUC', 'Balanced_Accuracy', 'G_mean']\n",
    "    for i, metric in enumerate(metrics_to_plot, 1):\n",
    "        plt.subplot(2, 2, i)\n",
    "        ax = sns.barplot(x=metric, y='Model', hue='Type', data=performance_df.head(10))\n",
    "        plt.title(f'Top 10 Models by {metric}', fontsize=14)\n",
    "        plt.xlim(0.5, 1.0)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/model_evaluation/performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Optimize thresholds for best models\n",
    "    print(\"Optimizing decision thresholds for best models...\")\n",
    "\n",
    "    # Select models for threshold optimization\n",
    "    threshold_models = {}\n",
    "\n",
    "    # Add top individual model\n",
    "    top_individual = best_models_f1[0]\n",
    "    threshold_models[f\"{top_individual['model_name']} ({top_individual['resampling_method']})\"] = top_individual['model']\n",
    "\n",
    "    # Add best ensemble\n",
    "    threshold_models[best_ensemble[0]] = best_ensemble[1]\n",
    "\n",
    "    # Add another top model\n",
    "    second_best = best_models_f1[1]\n",
    "    threshold_models[f\"{second_best['model_name']} ({second_best['resampling_method']})\"] = second_best['model']\n",
    "\n",
    "    # Metrics to optimize for\n",
    "    metrics_to_optimize = ['f1', 'balanced_accuracy', 'g_mean']\n",
    "\n",
    "    # Find optimal thresholds\n",
    "    optimized_thresholds = {}\n",
    "\n",
    "    for model_name, model in threshold_models.items():\n",
    "        optimized_thresholds[model_name] = {}\n",
    "        \n",
    "        for metric in metrics_to_optimize:\n",
    "            print(f\"\\nFinding optimal threshold for {model_name} based on {metric}...\")\n",
    "            \n",
    "            threshold = find_optimal_threshold(\n",
    "                model, scaled_data['standard']['val'], y_val, \n",
    "                metric=metric\n",
    "            )\n",
    "            \n",
    "            optimized_thresholds[model_name][metric] = threshold\n",
    "            \n",
    "            # Evaluate with the optimized threshold\n",
    "            metrics_dict = evaluate_model_comprehensive(\n",
    "                model, scaled_data['standard']['val'], y_val,\n",
    "                f\"{model_name} (optimized for {metric}, threshold={threshold:.2f})\",\n",
    "                threshold=threshold\n",
    "            )\n",
    "            \n",
    "            # Store the metrics\n",
    "            optimized_thresholds[model_name][f\"{metric}_metrics\"] = metrics_dict\n",
    "\n",
    "    # Create a threshold comparison dataframe\n",
    "    threshold_df = pd.DataFrame(columns=['Model', 'Optimization_Metric', 'Threshold', \n",
    "                                       'Accuracy', 'Balanced_Accuracy', 'Precision', \n",
    "                                       'Recall', 'F1', 'ROC_AUC', 'G_mean'])\n",
    "\n",
    "    for model_name, thresholds in optimized_thresholds.items():\n",
    "        for metric in metrics_to_optimize:\n",
    "            threshold = thresholds[metric]\n",
    "            metrics_dict = thresholds[f\"{metric}_metrics\"]\n",
    "            \n",
    "            threshold_df = pd.concat([threshold_df, pd.DataFrame([{\n",
    "                'Model': model_name,\n",
    "                'Optimization_Metric': metric,\n",
    "                'Threshold': threshold,\n",
    "                'Accuracy': metrics_dict['accuracy'],\n",
    "                'Balanced_Accuracy': metrics_dict['balanced_accuracy'],\n",
    "                'Precision': metrics_dict['precision'],\n",
    "                'Recall': metrics_dict['recall'],\n",
    "                'F1': metrics_dict['f1'],\n",
    "                'ROC_AUC': metrics_dict.get('roc_auc', np.nan),\n",
    "                'G_mean': metrics_dict['g_mean']\n",
    "            }])], ignore_index=True)\n",
    "\n",
    "    # Save the threshold comparison\n",
    "    threshold_df.to_csv('results/threshold_optimization.csv', index=False)\n",
    "\n",
    "    # Display the threshold comparison\n",
    "    print(\"\\nThreshold Optimization Results:\")\n",
    "    print(threshold_df)\n",
    "\n",
    "    # Visualize the threshold optimization results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.lineplot(x='Threshold', y='F1', hue='Model', data=threshold_df)\n",
    "    plt.title('F1 Score vs. Threshold by Model', fontsize=14)\n",
    "    plt.xlabel('Threshold', fontsize=12)\n",
    "    plt.ylabel('F1 Score', fontsize=12)\n",
    "    plt.ylim(0.5, 1.0)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('figures/model_evaluation/f1_vs_threshold.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Perform feature selection to identify the most important features\n",
    "    print(\"Performing feature selection to identify key predictors...\")\n",
    "\n",
    "    # Use the best model for feature importance\n",
    "    best_model_info = best_models_f1[0]\n",
    "    best_model = best_model_info['model']\n",
    "    best_model_name = best_model_info['model_name']\n",
    "\n",
    "    # Extract feature importance if available\n",
    "    feature_importance = plot_feature_importance_enhanced(\n",
    "        best_model, X_train.columns, \n",
    "        f\"{best_model_name} ({best_model_info['resampling_method']})\"\n",
    "    )\n",
    "\n",
    "    if feature_importance is not None:\n",
    "        # Select top features based on importance\n",
    "        top_features = feature_importance['Feature'].head(15).tolist()\n",
    "        \n",
    "        print(f\"\\nSelected top {len(top_features)} features:\")\n",
    "        print(top_features)\n",
    "        \n",
    "        # Create datasets with selected features\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_val_selected = X_val[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Scale the selected features\n",
    "        scaler_selected = StandardScaler()\n",
    "        X_train_selected_scaled = scaler_selected.fit_transform(X_train_selected)\n",
    "        X_val_selected_scaled = scaler_selected.transform(X_val_selected)\n",
    "        X_test_selected_scaled = scaler_selected.transform(X_test_selected)\n",
    "        \n",
    "        # Convert to DataFrames\n",
    "        X_train_selected_df = pd.DataFrame(X_train_selected_scaled, columns=top_features)\n",
    "        X_val_selected_df = pd.DataFrame(X_val_selected_scaled, columns=top_features)\n",
    "        X_test_selected_df = pd.DataFrame(X_test_selected_scaled, columns=top_features)\n",
    "        \n",
    "        # Apply best resampling technique\n",
    "        best_resampling = best_model_info['resampling_method']\n",
    "        print(f\"\\nApplying {best_resampling} to selected features...\")\n",
    "        \n",
    "        if best_resampling == 'smote':\n",
    "            resampler = SMOTE(random_state=SEED)\n",
    "        elif best_resampling == 'smotetomek':\n",
    "            resampler = SMOTETomek(random_state=SEED)\n",
    "        elif best_resampling == 'smoteenn':\n",
    "            resampler = SMOTEENN(random_state=SEED)\n",
    "        else:\n",
    "            resampler = SMOTE(random_state=SEED)\n",
    "        \n",
    "        X_train_selected_resampled, y_train_selected_resampled = resampler.fit_resample(\n",
    "            X_train_selected_scaled, y_train\n",
    "        )\n",
    "        \n",
    "        # Train the best model on selected features\n",
    "        print(f\"\\nTraining final model ({best_model_name}) on selected features with {best_resampling}...\")\n",
    "        \n",
    "        # Clone the best model\n",
    "        final_model = clone(best_model)\n",
    "        \n",
    "        # Train the final model\n",
    "        final_model.fit(X_train_selected_resampled, y_train_selected_resampled)\n",
    "        \n",
    "        # Find the optimal threshold for the final model\n",
    "        print(\"\\nOptimizing threshold for final model...\")\n",
    "        optimal_threshold = find_optimal_threshold(\n",
    "            final_model, X_val_selected_scaled, y_val, \n",
    "            metric='f1'\n",
    "        )\n",
    "        \n",
    "        # Evaluate the final model on validation set with optimal threshold\n",
    "        final_val_metrics = evaluate_model_comprehensive(\n",
    "            final_model, X_val_selected_scaled, y_val,\n",
    "            \"Final Model (Selected Features) - Validation\",\n",
    "            threshold=optimal_threshold\n",
    "        )\n",
    "        \n",
    "        # Evaluate the final model on test set\n",
    "        final_test_metrics = evaluate_model_comprehensive(\n",
    "            final_model, X_test_selected_scaled, y_test,\n",
    "            \"Final Model (Selected Features) - Test\",\n",
    "            threshold=optimal_threshold\n",
    "        )\n",
    "        \n",
    "        # Plot confusion matrix for test set\n",
    "        y_test_pred = (final_model.predict_proba(X_test_selected_scaled)[:, 1] >= optimal_threshold).astype(int)\n",
    "        test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "        plot_confusion_matrix_enhanced(\n",
    "            test_cm, \"Final Model (Selected Features) - Test\"\n",
    "        )\n",
    "        \n",
    "        # Print final test results\n",
    "        print(\"\\nFinal Test Results:\")\n",
    "        print(f\"Accuracy: {final_test_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Balanced Accuracy: {final_test_metrics['balanced_accuracy']:.4f}\")\n",
    "        print(f\"Precision: {final_test_metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {final_test_metrics['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {final_test_metrics['f1']:.4f}\")\n",
    "        print(f\"ROC AUC: {final_test_metrics['roc_auc']:.4f}\")\n",
    "        print(f\"G-mean: {final_test_metrics['g_mean']:.4f}\")\n",
    "        \n",
    "        # Save the final model\n",
    "        final_model_path = 'models/final_stroke_prediction_model.pkl'\n",
    "        with open(final_model_path, 'wb') as f:\n",
    "            pickle.dump(final_model, f)\n",
    "        \n",
    "        # Save the scaler\n",
    "        scaler_path = 'models/final_stroke_prediction_scaler.pkl'\n",
    "        with open(scaler_path, 'wb') as f:\n",
    "            pickle.dump(scaler_selected, f)\n",
    "        \n",
    "        # Save the selected features\n",
    "        with open('models/selected_features.pkl', 'wb') as f:\n",
    "            pickle.dump(top_features, f)\n",
    "        \n",
    "        # Save the optimal threshold\n",
    "        with open('models/optimal_threshold.pkl', 'wb') as f:\n",
    "            pickle.dump(optimal_threshold, f)\n",
    "        \n",
    "        print(f\"\\nFinal model saved to {final_model_path}\")\n",
    "        print(f\"Scaler saved to {scaler_path}\")\n",
    "        print(f\"Selected features: {top_features}\")\n",
    "        print(f\"Optimal threshold: {optimal_threshold:.4f}\")\n",
    "    else:\n",
    "        print(\"Could not extract feature importance. Using the best model without feature selection.\")\n",
    "        \n",
    "        # Use the best model directly\n",
    "        best_ensemble_name, best_ensemble_model = best_ensemble\n",
    "        \n",
    "        # Find the optimal threshold\n",
    "        print(\"\\nOptimizing threshold for best ensemble model...\")\n",
    "        optimal_threshold = find_optimal_threshold(\n",
    "            best_ensemble_model, scaled_data['standard']['val'], y_val, \n",
    "            metric='f1'\n",
    "        )\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        final_test_metrics = evaluate_model_comprehensive(\n",
    "            best_ensemble_model, scaled_data['standard']['test'], y_test,\n",
    "            f\"Best Ensemble Model ({best_ensemble_name}) - Test\",\n",
    "            threshold=optimal_threshold\n",
    "        )\n",
    "        \n",
    "        # Plot confusion matrix for test set\n",
    "        y_test_pred = (best_ensemble_model.predict_proba(scaled_data['standard']['test'])[:, 1] >= optimal_threshold).astype(int)\n",
    "        test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "        plot_confusion_matrix_enhanced(\n",
    "            test_cm, f\"Best Ensemble Model ({best_ensemble_name}) - Test\"\n",
    "        )\n",
    "        \n",
    "        # Save the best model\n",
    "        final_model_path = 'models/final_stroke_prediction_model.pkl'\n",
    "        with open(final_model_path, 'wb') as f:\n",
    "            pickle.dump(best_ensemble_model, f)\n",
    "        \n",
    "        # Save the scaler\n",
    "        scaler_path = 'models/final_stroke_prediction_scaler.pkl'\n",
    "        with open(scaler_path, 'wb') as f:\n",
    "            pickle.dump(scaled_data['standard']['scaler'], f)\n",
    "        \n",
    "        # Save the optimal threshold\n",
    "        with open('models/optimal_threshold.pkl', 'wb') as f:\n",
    "            pickle.dump(optimal_threshold, f)\n",
    "        \n",
    "        print(f\"\\nBest ensemble model saved to {final_model_path}\")\n",
    "        print(f\"Scaler saved to {scaler_path}\")\n",
    "        print(f\"Optimal threshold: {optimal_threshold:.4f}\")\n",
    "\n",
    "    # Test the prediction function on a sample from the test set\n",
    "    sample_size = min(5, len(X_test))\n",
    "    sample_indices = np.random.choice(len(X_test), sample_size, replace=False)\n",
    "    sample_data = X_test.iloc[sample_indices]\n",
    "    sample_labels = y_test.iloc[sample_indices]\n",
    "\n",
    "    print(f\"\\nTesting prediction function on {sample_size} random samples:\")\n",
    "    try:\n",
    "        predictions, probabilities = predict_stroke_risk(sample_data)\n",
    "        \n",
    "        print(\"\\nSample  True Label  Predicted  Probability\")\n",
    "        print(\"----------------------------------------\")\n",
    "        for i, (pred, prob, true) in enumerate(zip(predictions, probabilities, sample_labels)):\n",
    "            print(f\"{i+1:6d}  {true:10d}  {pred:9d}  {prob:.6f}\")\n",
    "        \n",
    "        print(\"\\nPrediction function works correctly!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error testing prediction function: {e}\")\n",
    "\n",
    "    # Create a summary of the model development process\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STROKE PREDICTION MODEL DEVELOPMENT SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(\"\\n1. Data Preparation and Engineering:\")\n",
    "    print(f\"   - Original dataset shape: {df.shape}\")\n",
    "    print(f\"   - Advanced feature engineering added {df_engineered.shape[1] - df.shape[1]} new features\")\n",
    "    print(\"   - Features included age transformations, risk scores, and interaction terms\")\n",
    "\n",
    "    print(\"\\n2. Class Imbalance Handling:\")\n",
    "    print(\"   - Original class distribution: {:.2f}% positive cases\".format(y_train.mean() * 100))\n",
    "    print(\"   - Applied multiple resampling techniques (SMOTE, SMOTETomek, SMOTEENN)\")\n",
    "    if 'best_model_info' in locals():\n",
    "        print(\"   - Best resampling method: \" + best_model_info['resampling_method'])\n",
    "    else:\n",
    "        print(\"   - Best resampling method: N/A\")\n",
    "\n",
    "    print(\"\\n3. Model Development:\")\n",
    "    print(\"   - Trained optimized models with imbalanced learning configurations\")\n",
    "    print(\"   - Created ensemble models to improve performance\")\n",
    "    if 'best_model_info' in locals():\n",
    "        print(f\"   - Best individual model: {best_model_info['model_name']} (F1={best_model_info['f1']:.4f})\")\n",
    "    else:\n",
    "        print(\"   - Best individual model: N/A\")\n",
    "    if 'best_ensemble' in locals() and 'best_ensemble_f1' in locals():\n",
    "        print(f\"   - Best ensemble model: {best_ensemble[0]} (F1={best_ensemble_f1:.4f})\")\n",
    "    else:\n",
    "        print(\"   - Best ensemble model: N/A\")\n",
    "\n",
    "    print(\"\\n4. Threshold Optimization:\")\n",
    "    print(\"   - Optimized decision thresholds for performance metrics (F1, G-mean)\")\n",
    "    if 'optimal_threshold' in locals():\n",
    "        print(f\"   - Optimal threshold: {optimal_threshold:.4f}\")\n",
    "    else:\n",
    "        print(\"   - Optimal threshold: N/A\")\n",
    "\n",
    "    print(\"\\n5. Feature Selection:\")\n",
    "    if 'top_features' in locals():\n",
    "        print(f\"   - Selected {len(top_features)} top features based on importance\")\n",
    "        print(f\"   - Key predictors: {', '.join(top_features[:5])}\")\n",
    "    else:\n",
    "        print(\"   - Used all available features\")\n",
    "\n",
    "    print(\"\\n6. Final Test Performance:\")\n",
    "    if 'final_test_metrics' in locals():\n",
    "        print(f\"   - Accuracy: {final_test_metrics['accuracy']:.4f}\")\n",
    "        print(f\"   - Balanced Accuracy: {final_test_metrics['balanced_accuracy']:.4f}\")\n",
    "        print(f\"   - Precision: {final_test_metrics['precision']:.4f}\")\n",
    "        print(f\"   - Recall: {final_test_metrics['recall']:.4f}\")\n",
    "        print(f\"   - F1 Score: {final_test_metrics['f1']:.4f}\")\n",
    "        print(f\"   - ROC AUC: {final_test_metrics['roc_auc']:.4f}\")\n",
    "        print(f\"   - G-mean: {final_test_metrics['g_mean']:.4f}\")\n",
    "    else:\n",
    "        print(\"   - Final test metrics not available\")\n",
    "\n",
    "    print(\"\\n7. Model Deployment:\")\n",
    "    print(f\"   - Final model saved to: models/final_stroke_prediction_model.pkl\")\n",
    "    print(f\"   - Prediction function created for new data\")\n",
    "    print(\"   - Ready for integration into applications\")\n",
    "\n",
    "    # Save the summary to a file\n",
    "    with open('results/model_development_summary.txt', 'w') as f:\n",
    "        f.write(\"STROKE PREDICTION MODEL DEVELOPMENT SUMMARY\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"1. Data Preparation and Engineering:\\n\")\n",
    "        f.write(f\"   - Original dataset shape: {df.shape}\\n\")\n",
    "        f.write(f\"   - Advanced feature engineering added {df_engineered.shape[1] - df.shape[1]} new features\\n\")\n",
    "        f.write(\"   - Features included age transformations, risk scores, and interaction terms\\n\\n\")\n",
    "        \n",
    "        f.write(\"2. Class Imbalance Handling:\\n\")\n",
    "        f.write(\"   - Original class distribution: {:.2f}% positive cases\\n\".format(y_train.mean() * 100))\n",
    "        f.write(\"   - Applied multiple resampling techniques (SMOTE, SMOTETomek, SMOTEENN)\\n\")\n",
    "        if 'best_model_info' in locals():\n",
    "            f.write(\"   - Best resampling method: \" + best_model_info['resampling_method'] + \"\\n\\n\")\n",
    "        else:\n",
    "            f.write(\"   - Best resampling method: N/A\\n\\n\")\n",
    "        \n",
    "        f.write(\"3. Model Development:\\n\")\n",
    "        f.write(\"   - Trained optimized models with imbalanced learning configurations\\n\")\n",
    "        f.write(\"   - Created ensemble models to improve performance\\n\")\n",
    "        if 'best_model_info' in locals():\n",
    "            f.write(f\"   - Best individual model: {best_model_info['model_name']} (F1={best_model_info['f1']:.4f})\\n\")\n",
    "        else:\n",
    "            f.write(\"   - Best individual model: N/A\\n\")\n",
    "        if 'best_ensemble' in locals() and 'best_ensemble_f1' in locals():\n",
    "            f.write(f\"   - Best ensemble model: {best_ensemble[0]} (F1={best_ensemble_f1:.4f})\\n\\n\")\n",
    "        else:\n",
    "            f.write(\"   - Best ensemble model: N/A\\n\\n\")\n",
    "        \n",
    "        f.write(\"4. Threshold Optimization:\\n\")\n",
    "        f.write(\"   - Optimized decision thresholds for performance metrics (F1, G-mean)\\n\")\n",
    "        if 'optimal_threshold' in locals():\n",
    "            f.write(f\"   - Optimal threshold: {optimal_threshold:.4f}\\n\\n\")\n",
    "        else:\n",
    "            f.write(\"   - Optimal threshold: N/A\\n\\n\")\n",
    "        \n",
    "        f.write(\"5. Feature Selection:\\n\")\n",
    "        if 'top_features' in locals():\n",
    "            f.write(f\"   - Selected {len(top_features)} top features based on importance\\n\")\n",
    "            f.write(f\"   - Key predictors: {', '.join(top_features[:5])}\\n\\n\")\n",
    "        else:\n",
    "            f.write(\"   - Used all available features\\n\\n\")\n",
    "        \n",
    "        f.write(\"6. Final Test Performance:\\n\")\n",
    "        if 'final_test_metrics' in locals():\n",
    "            f.write(f\"   - Accuracy: {final_test_metrics['accuracy']:.4f}\\n\")\n",
    "            f.write(f\"   - Balanced Accuracy: {final_test_metrics['balanced_accuracy']:.4f}\\n\")\n",
    "            f.write(f\"   - Precision: {final_test_metrics['precision']:.4f}\\n\")\n",
    "            f.write(f\"   - Recall: {final_test_metrics['recall']:.4f}\\n\")\n",
    "            f.write(f\"   - F1 Score: {final_test_metrics['f1']:.4f}\\n\")\n",
    "            f.write(f\"   - ROC AUC: {final_test_metrics['roc_auc']:.4f}\\n\")\n",
    "            f.write(f\"   - G-mean: {final_test_metrics['g_mean']:.4f}\\n\\n\")\n",
    "        else:\n",
    "            f.write(\"   - Final test metrics not available\\n\\n\")\n",
    "        \n",
    "        f.write(\"7. Model Deployment:\\n\")\n",
    "        f.write(f\"   - Final model saved to: models/final_stroke_prediction_model.pkl\\n\")\n",
    "        f.write(f\"   - Prediction function created for new data\\n\")\n",
    "        f.write(\"   - Ready for integration into applications\\n\")\n",
    "\n",
    "    print(\"\\nModel development summary saved to: results/model_development_summary.txt\")\n",
    "    print(\"\\nStroke prediction model development complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
